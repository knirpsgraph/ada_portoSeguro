{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T15:43:45.588722Z",
     "start_time": "2025-09-18T15:14:38.508051Z"
    }
   },
   "source": [
    "import os, sys, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    CWD = Path.cwd(); ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.data_loader import load_and_save_data\n",
    "from src.models import get_models\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Defaults for notebooks\n",
    "if 'get_ipython' in globals():\n",
    "    os.environ.setdefault(\"CV\", \"2\")  # WERT FÜR SCHNELLEN LAUF REDUZIERT\n",
    "    os.environ.setdefault(\"RND\", \"42\")\n",
    "    os.environ.setdefault(\"TE_CAT\", \"1\")\n",
    "    os.environ.setdefault(\"GBM_CHECK\", \"1\")\n",
    "    os.environ.setdefault(\"TRAIN_SAMPLE_N\", \"50000\") # WERT FÜR SCHNELLEN LAUF REDUZIERT\n",
    "    os.environ.setdefault(\"TE_ALPHA\", \"10\")\n",
    "\n",
    "# --- Runtime params\n",
    "RND       = int(os.getenv(\"RND\", \"42\"))\n",
    "CV        = int(os.getenv(\"CV\", \"2\"))\n",
    "C_LOGREG  = float(os.getenv(\"C\", \"1.0\"))\n",
    "N_SAMPLE  = int(os.getenv(\"TRAIN_SAMPLE_N\", \"50000\"))\n",
    "TE_CAT    = int(os.getenv(\"TE_CAT\", \"0\")) == 1\n",
    "GBM_CHECK = int(os.getenv(\"GBM_CHECK\", \"0\")) == 1\n",
    "TE_ALPHA  = float(os.getenv(\"TE_ALPHA\", \"10\"))\n",
    "\n",
    "# --- Helpers\n",
    "def ohe_fallback():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"infrequent_if_exist\",\n",
    "                             min_frequency=0.01, sparse_output=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def split_cols(cols):\n",
    "    cat = [c for c in cols if c.endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if c.endswith(\"_bin\")]\n",
    "    num = [c for c in cols if (c not in cat and c not in bin_ and c != \"target\")]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def fe_simple(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    b = [c for c in X.columns if c.endswith(\"_bin\")]\n",
    "    if b:\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1)\n",
    "    return X\n",
    "\n",
    "def build_pre(cat, bin_, num):\n",
    "    cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_fallback())])\n",
    "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "    bin_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    return ColumnTransformer([(\"cat\", cat_pipe, cat), (\"bin\", bin_pipe, bin_), (\"num\", num_pipe, num)], remainder=\"drop\")\n",
    "\n",
    "def make_feature_set(df, drop_calc=True, extra_drop=None, add_extras=True, drop_groups=None):\n",
    "    X = df.drop(columns=[\"target\"], errors=\"ignore\").copy().replace(-1, np.nan)\n",
    "    if drop_calc:\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\"ps_calc_\")], errors=\"ignore\")\n",
    "    if extra_drop:\n",
    "        X = X.drop(columns=[c for c in extra_drop if c in X.columns], errors=\"ignore\")\n",
    "    extras_cols = []\n",
    "    if add_extras:\n",
    "        X = fe_simple(X); extras_cols = [\"missing_count\", \"sum_all_bin\"]\n",
    "    if drop_groups:\n",
    "        cat, bin_, num = split_cols(X.columns)\n",
    "        if drop_groups.get(\"cat\"): X = X.drop(columns=cat, errors=\"ignore\")\n",
    "        if drop_groups.get(\"bin\"): X = X.drop(columns=bin_, errors=\"ignore\")\n",
    "        if drop_groups.get(\"num\"): X = X.drop(columns=num, errors=\"ignore\")\n",
    "        if drop_groups.get(\"extras\"): X = X.drop(columns=[c for c in extras_cols if c in X.columns], errors=\"ignore\")\n",
    "    return X\n",
    "\n",
    "def cv_scores_ohe(X, y, clf, C=1.0, CV=3, seed=RND):\n",
    "    cat, bin_, num = split_cols(X.columns)\n",
    "    pre = build_pre(cat, bin_, num)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for i, (tr, te) in enumerate(skf.split(X, y)):\n",
    "        print(f\"    - Fold {i+1}/{CV} wird trainiert ({int((i+1)/CV*100)}%)\")\n",
    "        m = pipe.fit(X.iloc[tr], y.iloc[tr])\n",
    "        proba[te] = m.predict_proba(X.iloc[te])[:, 1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "def _kfold_target_encode(train_cat: pd.DataFrame, y_tr: pd.Series, valid_cat: pd.DataFrame, n_splits=3, alpha=10, seed=RND):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = y_tr.mean()\n",
    "    tr_enc = pd.DataFrame(index=train_cat.index)\n",
    "    va_enc = pd.DataFrame(index=valid_cat.index)\n",
    "    for col in train_cat.columns:\n",
    "        oof = pd.Series(index=train_cat.index, dtype=float)\n",
    "        for tr_idx, va_idx in skf.split(train_cat, y_tr):\n",
    "            col_tr = train_cat.iloc[tr_idx][col]; y_sub = y_tr.iloc[tr_idx]\n",
    "            stats = y_sub.groupby(col_tr).agg(['mean','count'])\n",
    "            m = (stats['mean']*stats['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "            oof.iloc[va_idx] = train_cat.iloc[va_idx][col].map(m)\n",
    "        tr_enc[col] = oof.fillna(global_mean)\n",
    "        stats_full = y_tr.groupby(train_cat[col]).agg(['mean','count'])\n",
    "        m_full = (stats_full['mean']*stats_full['count'] + global_mean*alpha) / (stats_full['count'] + alpha)\n",
    "        va_enc[col] = valid_cat[col].map(m_full).fillna(global_mean)\n",
    "    tr_enc.columns = [f\"te_{c}\" for c in tr_enc.columns]\n",
    "    va_enc.columns = [f\"te_{c}\" for c in va_enc.columns]\n",
    "    return tr_enc, va_enc\n",
    "\n",
    "def _prep_te_blocks(X_tr, X_va):\n",
    "    cat, bin_, num = split_cols(X_tr.columns)\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_bin = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_cat = pd.DataFrame(imp_cat.fit_transform(X_tr[cat]) if cat else np.empty((len(X_tr),0)), columns=cat, index=X_tr.index)\n",
    "    Xva_cat = pd.DataFrame(imp_cat.transform(X_va[cat]) if cat else np.empty((len(X_va),0)), columns=cat, index=X_va.index)\n",
    "    Xtr_bin = pd.DataFrame(imp_bin.fit_transform(X_tr[bin_]) if bin_ else np.empty((len(X_tr),0)), columns=bin_, index=X_tr.index)\n",
    "    Xva_bin = pd.DataFrame(imp_bin.transform(X_va[bin_]) if bin_ else np.empty((len(X_va),0)), columns=bin_, index=X_va.index)\n",
    "    Xtr_num = pd.DataFrame(imp_num.fit_transform(X_tr[num]) if num else np.empty((len(X_tr),0)), columns=num, index=X_tr.index)\n",
    "    Xva_num = pd.DataFrame(imp_num.transform(X_va[num]) if num else np.empty((len(X_va),0)), columns=num, index=X_va.index)\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    if Xtr_num.shape[1]:\n",
    "        Xtr_num = pd.DataFrame(sc.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "        Xva_num = pd.DataFrame(sc.transform(Xva_num), columns=Xva_num.columns, index=Xva_num.index)\n",
    "    return (Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num)\n",
    "\n",
    "def cv_scores_te(X, y, clf, C=1.0, CV=3, seed=RND, alpha=TE_ALPHA):\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for i, (tr_idx, va_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"    - Fold {i+1}/{CV} wird trainiert ({int((i+1)/CV*100)}%)\")\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr, X_va)\n",
    "        if Xtr_cat.shape[1]:\n",
    "            tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr, Xva_cat, n_splits=CV, alpha=alpha, seed=seed)\n",
    "        else:\n",
    "            tr_te = pd.DataFrame(index=X_tr.index); va_te = pd.DataFrame(index=X_va.index)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf.fit(Xtr_fin, y_tr)\n",
    "        proba[va_idx] = clf.predict_proba(Xva_fin)[:,1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "\n",
    "def holdout_gbm_check(X_tr, y_tr, X_te, y_te, seed=RND):\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "    except Exception:\n",
    "        print(\"[GBM] LightGBM not available – skip.\")\n",
    "        return None\n",
    "    clf = LGBMClassifier(n_estimators=300, learning_rate=0.1, num_leaves=31,\n",
    "                         subsample=0.8, colsample_bytree=0.8, reg_lambda=0.0,\n",
    "                         random_state=seed, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    p = clf.predict_proba(X_te)[:,1]\n",
    "    return float(roc_auc_score(y_te, p)), float(average_precision_score(y_te, p))\n",
    "\n",
    "def main():\n",
    "    reports = ROOT/\"reports_Hany\"; reports.mkdir(parents=True, exist_ok=True)\n",
    "    models_to_test = get_models(C_LOGREG, RND)\n",
    "\n",
    "    if N_SAMPLE > 0 and N_SAMPLE < 5000:\n",
    "        print(f\"--- Führe einen Testlauf mit N_SAMPLE={N_SAMPLE:,} durch ---\")\n",
    "\n",
    "    df_all = load_and_save_data().replace(-1, np.nan)\n",
    "    n_rows_total = len(df_all)\n",
    "    df = df_all\n",
    "    if N_SAMPLE and N_SAMPLE < len(df_all):\n",
    "        df = df_all.sample(N_SAMPLE, random_state=RND).sort_index()\n",
    "    y = df[\"target\"].astype(int)\n",
    "\n",
    "    X_tr_all, X_te_all, y_tr, y_te = train_test_split(\n",
    "        df.drop(columns=[\"target\"]), y, test_size=0.2, stratify=y, random_state=RND\n",
    "    )\n",
    "    df_tr = pd.concat([X_tr_all, y_tr], axis=1)\n",
    "    df_te = pd.concat([X_te_all, y_te], axis=1)\n",
    "\n",
    "    split_indices = {\"train\": df_tr.index.tolist(), \"test\": df_te.index.tolist()}\n",
    "    (reports/\"split_indices.json\").write_text(json.dumps(split_indices, indent=2))\n",
    "\n",
    "    configs = [\n",
    "        {\"name\":\"all_features\", \"drop_calc\":False, \"extra_drop\":[], \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+opt+extras\", \"drop_calc\":True, \"extra_drop\":[\"ps_ind_14\",\"ps_car_10_cat\"], \"add_extras\":True},\n",
    "        {\"name\":\"drop_calc_only\", \"drop_calc\":True, \"extra_drop\":[], \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+extras\", \"drop_calc\":True, \"extra_drop\":[], \"add_extras\":True},\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for model_name, model_clf in models_to_test.items():\n",
    "        print(f\"Starte Kreuzvalidierung für Modell: {model_name}...\")\n",
    "        for cfg in configs:\n",
    "            print(f\"  Konfiguration: {cfg['name']}...\")\n",
    "            X_tr_cfg = make_feature_set(df_tr, drop_calc=cfg[\"drop_calc\"], extra_drop=cfg[\"extra_drop\"], add_extras=cfg[\"add_extras\"])\n",
    "\n",
    "            if isinstance(model_clf, (RandomForestClassifier, SVC)):\n",
    "                auc_cv, pr_cv = cv_scores_ohe(X_tr_cfg, y_tr.loc[X_tr_cfg.index], model_clf, C=C_LOGREG, CV=CV)\n",
    "            elif TE_CAT:\n",
    "                auc_cv, pr_cv = cv_scores_te(X_tr_cfg, y_tr.loc[X_tr_cfg.index], C=C_LOGREG, CV=CV, clf=model_clf)\n",
    "            else:\n",
    "                auc_cv, pr_cv = cv_scores_ohe(X_tr_cfg, y_tr.loc[X_tr_cfg.index], C=C_LOGREG, CV=CV)\n",
    "\n",
    "            rows.append({\n",
    "                \"model_name\": model_name, \"config_name\": cfg[\"name\"], \"n_features\": int(X_tr_cfg.shape[1]),\n",
    "                \"cv_auc\": float(auc_cv), \"cv_pr_auc\": float(pr_cv),\n",
    "                \"drop_calc\": cfg[\"drop_calc\"], \"extra_drop\": cfg[\"extra_drop\"],\n",
    "                \"add_extras\": cfg[\"add_extras\"], \"te_cat\": TE_CAT\n",
    "            })\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values([\"cv_auc\",\"cv_pr_auc\"], ascending=False)\n",
    "    res_path = reports/\"feature_gate_scores.csv\"; res.to_csv(res_path, index=False)\n",
    "\n",
    "    best_cv = res.iloc[0].to_dict()\n",
    "    X_tr_best = make_feature_set(df_tr, drop_calc=best_cv[\"drop_calc\"], extra_drop=best_cv[\"extra_drop\"], add_extras=best_cv[\"add_extras\"])\n",
    "    X_te_best = make_feature_set(df_te, drop_calc=best_cv[\"drop_calc\"], extra_drop=best_cv[\"extra_drop\"], add_extras=best_cv[\"add_extras\"])\n",
    "\n",
    "    (reports/\"features_selected.csv\").write_text(\n",
    "        pd.Series(pd.Index(X_tr_best.columns), name=\"raw_feature\").to_csv(index=False)\n",
    "    )\n",
    "\n",
    "    best_model_name = best_cv[\"model_name\"]\n",
    "    best_model_clf = models_to_test[best_model_name]\n",
    "\n",
    "    if TE_CAT:\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr_best, X_te_best)\n",
    "        tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr.loc[X_tr_best.index], Xva_cat, n_splits=CV, alpha=TE_ALPHA, seed=RND)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf_best = models_to_test[best_model_name]\n",
    "        if hasattr(clf_best, 'C'): clf_best.C = C_LOGREG\n",
    "        clf_best.fit(Xtr_fin, y_tr.loc[X_tr_best.index])\n",
    "        proba_best = clf_best.predict_proba(Xva_fin)[:,1]\n",
    "    else:\n",
    "        cat_b, bin_b, num_b = split_cols(X_tr_best.columns)\n",
    "        pre_b = build_pre(cat_b, bin_b, num_b)\n",
    "        pipe_b = Pipeline([(\"pre\", pre_b), (\"clf\", best_model_clf)])\n",
    "        m_b = pipe_b.fit(X_tr_best, y_tr.loc[X_tr_best.index])\n",
    "        proba_best = m_b.predict_proba(X_te_best)[:,1]\n",
    "\n",
    "    y_true_best = y_te.loc[X_te_best.index]\n",
    "\n",
    "    X_tr_allF = make_feature_set(df_tr, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    X_te_allF = make_feature_set(df_te, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    catA, binA, numA = split_cols(X_tr_allF.columns)\n",
    "    pipe_all = Pipeline([(\"pre\", build_pre(catA, binA, numA)),\n",
    "                             (\"clf\", models_to_test[\"LogisticRegression\"])])\n",
    "    m_all = pipe_all.fit(X_tr_allF, y_tr.loc[X_tr_allF.index])\n",
    "    proba_all = m_all.predict_proba(X_te_allF)[:,1]\n",
    "\n",
    "    hold_auc_best = roc_auc_score(y_true_best, proba_best)\n",
    "    hold_pr_best = average_precision_score(y_true_best, proba_best)\n",
    "    hold_auc_all = roc_auc_score(y_te.loc[X_te_allF.index], proba_all)\n",
    "    hold_pr_all = average_precision_score(y_te.loc[X_te_allF.index], proba_all)\n",
    "\n",
    "    prec_b, rec_b, _ = precision_recall_curve(y_true_best, proba_best)\n",
    "    prec_a, rec_a, _ = precision_recall_curve(y_te.loc[X_te_allF.index], proba_all)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(rec_b, prec_b, label=f\"Best ({best_model_name}, AP={hold_pr_best:.3f}, AUC={hold_auc_best:.3f})\")\n",
    "    plt.plot(rec_a, prec_a, label=f\"All-features (LR, AP={hold_pr_all:.3f}, AUC={hold_auc_all:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Holdout Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports/\"holdout_pr_curve.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    gbm_out = None\n",
    "    if GBM_CHECK:\n",
    "        catB, binB, numB = split_cols(X_tr_best.columns); preB = build_pre(catB, binB, numB)\n",
    "        XtrB = preB.fit_transform(X_tr_best); XvaB = preB.transform(X_te_best)\n",
    "        gbm_best = holdout_gbm_check(XtrB, y_tr.loc[X_tr_best.index], XvaB, y_true_best)\n",
    "        catC, binC, numC = split_cols(X_tr_allF.columns); preC = build_pre(catC, binC, numC)\n",
    "        XtrC = preC.fit_transform(X_tr_allF); XvaC = preC.transform(X_te_allF)\n",
    "        gbm_all = holdout_gbm_check(XtrC, y_tr.loc[X_tr_allF.index], XvaC, y_te.loc[X_te_allF.index])\n",
    "        gbm_out = {\"best\": gbm_best, \"all\": gbm_all}\n",
    "\n",
    "    meta = {\n",
    "        \"random_state\": RND, \"cv_splits\": CV, \"C\": C_LOGREG,\n",
    "        \"n_rows_total\": int(n_rows_total), \"sample_n\": int(len(df)),\n",
    "        \"te_cat\": TE_CAT, \"te_alpha\": TE_ALPHA, \"gbm_check\": bool(GBM_CHECK),\n",
    "        \"scores_path\": str(res_path),\n",
    "        \"features_path\": str(reports/\"features_selected.csv\"),\n",
    "        \"split_indices_path\": str(reports/\"split_indices.json\"),\n",
    "        \"pr_curve_path\": str(reports/\"holdout_pr_curve.png\"),\n",
    "        \"best_by_cv\": best_cv,\n",
    "        \"holdout_scores\": {\n",
    "            \"best_auc\": float(hold_auc_best), \"best_pr_auc\": float(hold_pr_best),\n",
    "            \"all_auc\": float(hold_auc_all), \"all_pr_auc\": float(hold_pr_all)\n",
    "        },\n",
    "        \"gbm_holdout\": gbm_out\n",
    "    }\n",
    "    (reports/\"feature_gate_meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    print(\"\\nFEATURE-GATE done.\")\n",
    "    print(f\"Train n={len(df_tr):,}, Holdout n={len(df_te):,}, CV={CV}, C={C_LOGREG}, TE_CAT={int(TE_CAT)}\")\n",
    "    print(\"Scores (CV):\\n\" + res.head(10).to_string(index=False))\n",
    "    print(f\"\\nHoldout (Best by CV): AUC={hold_auc_best:.4f}  PR-AUC={hold_pr_best:.4f}\")\n",
    "    print(f\"Holdout (All-features): AUC={hold_auc_all:.4f}  PR-AUC={hold_pr_all:.4f}\")\n",
    "    print(\"\\nArtifacts:\")\n",
    "    print(\"→ features_selected.csv\")\n",
    "    print(\"→ split_indices.json\")\n",
    "    print(\"→ holdout_pr_curve.png\")\n",
    "    print(\"(+ feature_gate_scores.csv, feature_gate_meta.json)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Datensatz aus dem Cache.\n",
      "Starte Kreuzvalidierung für Modell: LogisticRegression...\n",
      "  Konfiguration: all_features...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+opt+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc_only...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "Starte Kreuzvalidierung für Modell: RandomForestClassifier...\n",
      "  Konfiguration: all_features...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+opt+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc_only...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "Starte Kreuzvalidierung für Modell: SVM...\n",
      "  Konfiguration: all_features...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+opt+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc_only...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "  Konfiguration: drop_calc+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "Starte Kreuzvalidierung für Modell: LightGBM...\n",
      "  Konfiguration: all_features...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "[LightGBM] [Info] Number of positive: 750, number of negative: 19250\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1473\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037500 -> initscore=-3.245193\n",
      "[LightGBM] [Info] Start training from score -3.245193\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "[LightGBM] [Info] Number of positive: 749, number of negative: 19251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002827 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1467\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 55\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037450 -> initscore=-3.246579\n",
      "[LightGBM] [Info] Start training from score -3.246579\n",
      "  Konfiguration: drop_calc+opt+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "[LightGBM] [Info] Number of positive: 750, number of negative: 19250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001677 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1287\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037500 -> initscore=-3.245193\n",
      "[LightGBM] [Info] Start training from score -3.245193\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "[LightGBM] [Info] Number of positive: 749, number of negative: 19251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1283\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037450 -> initscore=-3.246579\n",
      "[LightGBM] [Info] Start training from score -3.246579\n",
      "  Konfiguration: drop_calc_only...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "[LightGBM] [Info] Number of positive: 750, number of negative: 19250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001639 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1285\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037500 -> initscore=-3.245193\n",
      "[LightGBM] [Info] Start training from score -3.245193\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "[LightGBM] [Info] Number of positive: 749, number of negative: 19251\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1282\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037450 -> initscore=-3.246579\n",
      "[LightGBM] [Info] Start training from score -3.246579\n",
      "  Konfiguration: drop_calc+extras...\n",
      "    - Fold 1/2 wird trainiert (50%)\n",
      "[LightGBM] [Info] Number of positive: 750, number of negative: 19250\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1298\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037500 -> initscore=-3.245193\n",
      "[LightGBM] [Info] Start training from score -3.245193\n",
      "    - Fold 2/2 wird trainiert (100%)\n",
      "[LightGBM] [Info] Number of positive: 749, number of negative: 19251\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1295\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037450 -> initscore=-3.246579\n",
      "[LightGBM] [Info] Start training from score -3.246579\n",
      "[LightGBM] [Info] Number of positive: 1499, number of negative: 38501\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1146\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037475 -> initscore=-3.245886\n",
      "[LightGBM] [Info] Start training from score -3.245886\n",
      "[LightGBM] [Info] Number of positive: 1499, number of negative: 38501\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004400 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1338\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 130\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.037475 -> initscore=-3.245886\n",
      "[LightGBM] [Info] Start training from score -3.245886\n",
      "\n",
      "FEATURE-GATE done.\n",
      "Train n=40,000, Holdout n=10,000, CV=2, C=1.0, TE_CAT=1\n",
      "Scores (CV):\n",
      "            model_name          config_name  n_features   cv_auc  cv_pr_auc  drop_calc                 extra_drop  add_extras  te_cat\n",
      "    LogisticRegression       drop_calc_only          37 0.601683   0.057119       True                         []       False    True\n",
      "    LogisticRegression     drop_calc+extras          39 0.600948   0.056611       True                         []        True    True\n",
      "    LogisticRegression drop_calc+opt+extras          37 0.600917   0.056601       True [ps_ind_14, ps_car_10_cat]        True    True\n",
      "              LightGBM       drop_calc_only          37 0.597380   0.055368       True                         []       False    True\n",
      "              LightGBM     drop_calc+extras          39 0.594444   0.053227       True                         []        True    True\n",
      "    LogisticRegression         all_features          57 0.594274   0.055915      False                         []       False    True\n",
      "              LightGBM drop_calc+opt+extras          37 0.593881   0.055226       True [ps_ind_14, ps_car_10_cat]        True    True\n",
      "RandomForestClassifier       drop_calc_only          37 0.591986   0.050285       True                         []       False    True\n",
      "RandomForestClassifier     drop_calc+extras          39 0.586168   0.050454       True                         []        True    True\n",
      "RandomForestClassifier drop_calc+opt+extras          37 0.581716   0.049191       True [ps_ind_14, ps_car_10_cat]        True    True\n",
      "\n",
      "Holdout (Best by CV): AUC=0.5921  PR-AUC=0.0613\n",
      "Holdout (All-features): AUC=0.5956  PR-AUC=0.0636\n",
      "\n",
      "Artifacts:\n",
      "→ features_selected.csv\n",
      "→ split_indices.json\n",
      "→ holdout_pr_curve.png\n",
      "(+ feature_gate_scores.csv, feature_gate_meta.json)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338bdc952d42b7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T14:19:11.322343Z",
     "start_time": "2025-09-18T14:19:11.310287Z"
    }
   },
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56ddf6a-2bf9-4943-b523-d2bd05754d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (porto_seguro_env)",
   "language": "python",
   "name": "porto_seguro_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
