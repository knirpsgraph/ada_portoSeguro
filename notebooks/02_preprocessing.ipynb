{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Datensatz aus dem Cache.\n",
      "[LightGBM] [Info] Number of positive: 7316, number of negative: 192684\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1190\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 110\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036580 -> initscore=-3.270988\n",
      "[LightGBM] [Info] Start training from score -3.270988\n",
      "[LightGBM] [Info] Number of positive: 7316, number of negative: 192684\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1381\n",
      "[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 131\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036580 -> initscore=-3.270988\n",
      "[LightGBM] [Info] Start training from score -3.270988\n",
      "\n",
      "FEATURE-GATE done.\n",
      "Train n=200,000, Holdout n=50,000, CV=3, C=1.0, TE_CAT=1\n",
      "Scores (CV):\n",
      "                name  n_features   cv_auc  cv_pr_auc  drop_calc                 extra_drop  add_extras  te_cat\n",
      "drop_calc+opt+extras          37 0.618422   0.058808       True [ps_ind_14, ps_car_10_cat]        True    True\n",
      "    drop_calc+extras          39 0.618302   0.058500       True                         []        True    True\n",
      "      drop_calc_only          37 0.618155   0.058449       True                         []       False    True\n",
      "        all_features          57 0.616991   0.058386      False                         []       False    True\n",
      "\n",
      "Holdout (Best by CV): AUC=0.6311  PR-AUC=0.0633\n",
      "Holdout (All-features): AUC=0.6259  PR-AUC=0.0624\n",
      "\n",
      "Artifacts:\n",
      "→ features_selected.csv\n",
      "→ split_indices.json\n",
      "→ holdout_pr_curve.png\n",
      "(+ feature_gate_scores.csv, feature_gate_meta.json)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Repo root (notebook/script-safe)\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    CWD = Path.cwd(); ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_recall_curve\n",
    ")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Defaults for notebooks\n",
    "if 'get_ipython' in globals():\n",
    "    os.environ.setdefault(\"CV\", \"3\")\n",
    "    os.environ.setdefault(\"RND\", \"42\")\n",
    "    os.environ.setdefault(\"TE_CAT\", \"1\")\n",
    "    os.environ.setdefault(\"GBM_CHECK\", \"1\")\n",
    "    os.environ.setdefault(\"TRAIN_SAMPLE_N\", \"250000\")\n",
    "    os.environ.setdefault(\"TE_ALPHA\", \"10\")\n",
    "\n",
    "# --- Runtime params\n",
    "RND       = int(os.getenv(\"RND\", \"42\"))\n",
    "CV        = int(os.getenv(\"CV\", \"3\"))\n",
    "C_LOGREG  = float(os.getenv(\"C\", \"1.0\"))\n",
    "N_SAMPLE  = int(os.getenv(\"TRAIN_SAMPLE_N\", \"250000\"))  # 0 = all\n",
    "TE_CAT    = int(os.getenv(\"TE_CAT\", \"0\")) == 1\n",
    "GBM_CHECK = int(os.getenv(\"GBM_CHECK\", \"0\")) == 1\n",
    "TE_ALPHA  = float(os.getenv(\"TE_ALPHA\", \"10\"))\n",
    "\n",
    "# --- Helpers\n",
    "def ohe_fallback():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"infrequent_if_exist\",\n",
    "                             min_frequency=0.01, sparse_output=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def split_cols(cols):\n",
    "    cat = [c for c in cols if c.endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if c.endswith(\"_bin\")]\n",
    "    num  = [c for c in cols if (c not in cat and c not in bin_ and c != \"target\")]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def fe_simple(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    b = [c for c in X.columns if c.endswith(\"_bin\")]\n",
    "    if b:\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1)\n",
    "    return X\n",
    "\n",
    "def build_pre(cat, bin_, num):\n",
    "    cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_fallback())])\n",
    "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "    bin_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    return ColumnTransformer([(\"cat\", cat_pipe, cat), (\"bin\", bin_pipe, bin_), (\"num\", num_pipe, num)], remainder=\"drop\")\n",
    "\n",
    "def make_feature_set(df, drop_calc=True, extra_drop=None, add_extras=True, drop_groups=None):\n",
    "    X = df.drop(columns=[\"target\"], errors=\"ignore\").copy().replace(-1, np.nan)\n",
    "    if drop_calc:\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\"ps_calc_\")], errors=\"ignore\")\n",
    "    if extra_drop:\n",
    "        X = X.drop(columns=[c for c in extra_drop if c in X.columns], errors=\"ignore\")\n",
    "    extras_cols = []\n",
    "    if add_extras:\n",
    "        X = fe_simple(X); extras_cols = [\"missing_count\", \"sum_all_bin\"]\n",
    "    if drop_groups:\n",
    "        cat, bin_, num = split_cols(X.columns)\n",
    "        if drop_groups.get(\"cat\"):    X = X.drop(columns=cat, errors=\"ignore\")\n",
    "        if drop_groups.get(\"bin\"):    X = X.drop(columns=bin_, errors=\"ignore\")\n",
    "        if drop_groups.get(\"num\"):    X = X.drop(columns=num, errors=\"ignore\")\n",
    "        if drop_groups.get(\"extras\"): X = X.drop(columns=[c for c in extras_cols if c in X.columns], errors=\"ignore\")\n",
    "    return X\n",
    "\n",
    "def cv_scores_ohe(X, y, C=1.0, CV=3, seed=RND):\n",
    "    cat, bin_, num = split_cols(X.columns)\n",
    "    pre  = build_pre(cat, bin_, num)\n",
    "    clf  = LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for tr, te in skf.split(X, y):\n",
    "        m = pipe.fit(X.iloc[tr], y.iloc[tr])\n",
    "        proba[te] = m.predict_proba(X.iloc[te])[:, 1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "# --- Target Encoding\n",
    "def _kfold_target_encode(train_cat: pd.DataFrame, y_tr: pd.Series, valid_cat: pd.DataFrame, n_splits=3, alpha=10, seed=RND):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = y_tr.mean()\n",
    "    tr_enc = pd.DataFrame(index=train_cat.index)\n",
    "    va_enc = pd.DataFrame(index=valid_cat.index)\n",
    "    for col in train_cat.columns:\n",
    "        oof = pd.Series(index=train_cat.index, dtype=float)\n",
    "        for tr_idx, va_idx in skf.split(train_cat, y_tr):\n",
    "            col_tr = train_cat.iloc[tr_idx][col]; y_sub = y_tr.iloc[tr_idx]\n",
    "            stats = y_sub.groupby(col_tr).agg(['mean','count'])\n",
    "            m = (stats['mean']*stats['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "            oof.iloc[va_idx] = train_cat.iloc[va_idx][col].map(m)\n",
    "        tr_enc[col] = oof.fillna(global_mean)\n",
    "        stats_full = y_tr.groupby(train_cat[col]).agg(['mean','count'])\n",
    "        m_full = (stats_full['mean']*stats_full['count'] + global_mean*alpha) / (stats_full['count'] + alpha)\n",
    "        va_enc[col] = valid_cat[col].map(m_full).fillna(global_mean)\n",
    "    tr_enc.columns = [f\"te_{c}\" for c in tr_enc.columns]\n",
    "    va_enc.columns = [f\"te_{c}\" for c in va_enc.columns]\n",
    "    return tr_enc, va_enc\n",
    "\n",
    "def _prep_te_blocks(X_tr, X_va):\n",
    "    cat, bin_, num = split_cols(X_tr.columns)\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_bin = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_cat = pd.DataFrame(imp_cat.fit_transform(X_tr[cat]) if cat else np.empty((len(X_tr),0)), columns=cat, index=X_tr.index)\n",
    "    Xva_cat = pd.DataFrame(imp_cat.transform(X_va[cat]) if cat else np.empty((len(X_va),0)), columns=cat, index=X_va.index)\n",
    "    Xtr_bin = pd.DataFrame(imp_bin.fit_transform(X_tr[bin_]) if bin_ else np.empty((len(X_tr),0)), columns=bin_, index=X_tr.index)\n",
    "    Xva_bin = pd.DataFrame(imp_bin.transform(X_va[bin_]) if bin_ else np.empty((len(X_va),0)), columns=bin_, index=X_va.index)\n",
    "    Xtr_num = pd.DataFrame(imp_num.fit_transform(X_tr[num]) if num else np.empty((len(X_tr),0)), columns=num, index=X_tr.index)\n",
    "    Xva_num = pd.DataFrame(imp_num.transform(X_va[num]) if num else np.empty((len(X_va),0)), columns=num, index=X_va.index)\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    if Xtr_num.shape[1]:\n",
    "        Xtr_num = pd.DataFrame(sc.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "        Xva_num = pd.DataFrame(sc.transform(Xva_num), columns=Xva_num.columns, index=Xva_num.index)\n",
    "    return (Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num)\n",
    "\n",
    "def cv_scores_te(X, y, C=1.0, CV=3, seed=RND, alpha=TE_ALPHA):\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr, X_va)\n",
    "        if Xtr_cat.shape[1]:\n",
    "            tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr, Xva_cat, n_splits=CV, alpha=alpha, seed=seed)\n",
    "        else:\n",
    "            tr_te = pd.DataFrame(index=X_tr.index); va_te = pd.DataFrame(index=X_va.index)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "        clf.fit(Xtr_fin, y_tr)\n",
    "        proba[va_idx] = clf.predict_proba(Xva_fin)[:,1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "# --- Optional: LightGBM check \n",
    "def holdout_gbm_check(X_tr, y_tr, X_te, y_te, seed=RND):\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "    except Exception:\n",
    "        print(\"[GBM] LightGBM not available – skip.\")\n",
    "        return None\n",
    "    clf = LGBMClassifier(n_estimators=300, learning_rate=0.1, num_leaves=31,\n",
    "                         subsample=0.8, colsample_bytree=0.8, reg_lambda=0.0,\n",
    "                         random_state=seed, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    p = clf.predict_proba(X_te)[:,1]\n",
    "    return float(roc_auc_score(y_te, p)), float(average_precision_score(y_te, p))\n",
    "\n",
    "# --- Main\n",
    "def main():\n",
    "    reports = ROOT/\"reports\"; reports.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load data\n",
    "    df_all = load_and_save_data().replace(-1, np.nan)\n",
    "    n_rows_total = len(df_all)\n",
    "    df = df_all\n",
    "    if N_SAMPLE and N_SAMPLE < len(df_all):\n",
    "        df = df_all.sample(N_SAMPLE, random_state=RND).sort_index()\n",
    "    y  = df[\"target\"].astype(int)\n",
    "\n",
    "    # Consistent holdout split (export indices)\n",
    "    X_tr_all, X_te_all, y_tr, y_te = train_test_split(\n",
    "        df.drop(columns=[\"target\"]), y, test_size=0.2, stratify=y, random_state=RND\n",
    "    )\n",
    "    df_tr = pd.concat([X_tr_all, y_tr], axis=1)\n",
    "    df_te = pd.concat([X_te_all, y_te], axis=1)\n",
    "\n",
    "    split_indices = {\"train\": df_tr.index.tolist(), \"test\": df_te.index.tolist()}\n",
    "    (reports/\"split_indices.json\").write_text(json.dumps(split_indices, indent=2))\n",
    "\n",
    "    # Candidate configs\n",
    "    configs = [\n",
    "        {\"name\":\"all_features\",           \"drop_calc\":False, \"extra_drop\":[],                           \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+opt+extras\",  \"drop_calc\":True,  \"extra_drop\":[\"ps_ind_14\",\"ps_car_10_cat\"], \"add_extras\":True},\n",
    "        {\"name\":\"drop_calc_only\",        \"drop_calc\":True,  \"extra_drop\":[],                           \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+extras\",      \"drop_calc\":True,  \"extra_drop\":[],                           \"add_extras\":True},\n",
    "    ]\n",
    "\n",
    "    # CV scores per config\n",
    "    rows = []\n",
    "    for cfg in configs:\n",
    "        X_tr_cfg = make_feature_set(df_tr, drop_calc=cfg[\"drop_calc\"], extra_drop=cfg[\"extra_drop\"], add_extras=cfg[\"add_extras\"])\n",
    "        if TE_CAT:\n",
    "            auc_cv, pr_cv = cv_scores_te(X_tr_cfg, y_tr.loc[X_tr_cfg.index], C=C_LOGREG, CV=CV)\n",
    "        else:\n",
    "            auc_cv, pr_cv = cv_scores_ohe(X_tr_cfg, y_tr.loc[X_tr_cfg.index], C=C_LOGREG, CV=CV)\n",
    "        rows.append({\n",
    "            \"name\":cfg[\"name\"], \"n_features\":int(X_tr_cfg.shape[1]),\n",
    "            \"cv_auc\":float(auc_cv), \"cv_pr_auc\":float(pr_cv),\n",
    "            \"drop_calc\":cfg[\"drop_calc\"], \"extra_drop\":cfg[\"extra_drop\"],\n",
    "            \"add_extras\":cfg[\"add_extras\"], \"te_cat\": TE_CAT\n",
    "        })\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values([\"cv_auc\",\"cv_pr_auc\"], ascending=False)\n",
    "    res_path = reports/\"feature_gate_scores.csv\"; res.to_csv(res_path, index=False)\n",
    "\n",
    "    # --- Select BEST BY CV AUC (tie-breaker: PR-AUC)\n",
    "    best_cv = res.iloc[0].to_dict()\n",
    "    X_tr_best = make_feature_set(df_tr, drop_calc=best_cv[\"drop_calc\"], extra_drop=best_cv[\"extra_drop\"], add_extras=best_cv[\"add_extras\"])\n",
    "    X_te_best = make_feature_set(df_te, drop_calc=best_cv[\"drop_calc\"], extra_drop=best_cv[\"extra_drop\"], add_extras=best_cv[\"add_extras\"])\n",
    "\n",
    "    # Export features (exactly those used by best CV AUC)\n",
    "    (reports/\"features_selected.csv\").write_text(\n",
    "        pd.Series(pd.Index(X_tr_best.columns), name=\"raw_feature\").to_csv(index=False)\n",
    "    )\n",
    "\n",
    "    # --- Holdout eval (best vs. all_features) for plotting\n",
    "    # Best\n",
    "    if TE_CAT:\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr_best, X_te_best)\n",
    "        tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr.loc[X_tr_best.index], Xva_cat, n_splits=CV, alpha=TE_ALPHA, seed=RND)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf_best = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C_LOGREG, class_weight=\"balanced\", max_iter=4000, random_state=RND)\n",
    "        clf_best.fit(Xtr_fin, y_tr.loc[X_tr_best.index])\n",
    "        proba_best = clf_best.predict_proba(Xva_fin)[:,1]\n",
    "    else:\n",
    "        cat_b, bin_b, num_b = split_cols(X_tr_best.columns)\n",
    "        pipe_best = Pipeline([(\"pre\", build_pre(cat_b, bin_b, num_b)),\n",
    "                              (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C_LOGREG, class_weight=\"balanced\", max_iter=4000, random_state=RND))])\n",
    "        m_best = pipe_best.fit(X_tr_best, y_tr.loc[X_tr_best.index])\n",
    "        proba_best = m_best.predict_proba(X_te_best)[:,1]\n",
    "\n",
    "    y_true_best = y_te.loc[X_te_best.index]\n",
    "\n",
    "    # All-features baseline\n",
    "    X_tr_allF = make_feature_set(df_tr, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    X_te_allF = make_feature_set(df_te, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    if TE_CAT:\n",
    "        Xtr_catA, Xva_catA, Xtr_binA, Xva_binA, Xtr_numA, Xva_numA = _prep_te_blocks(X_tr_allF, X_te_allF)\n",
    "        tr_teA, va_teA = _kfold_target_encode(Xtr_catA, y_tr.loc[X_tr_allF.index], Xva_catA, n_splits=CV, alpha=TE_ALPHA, seed=RND)\n",
    "        Xtr_all_fin = pd.concat([Xtr_numA, Xtr_binA, tr_teA], axis=1)\n",
    "        Xva_all_fin = pd.concat([Xva_numA, Xva_binA, va_teA], axis=1)\n",
    "        clf_all = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C_LOGREG, class_weight=\"balanced\", max_iter=4000, random_state=RND)\n",
    "        clf_all.fit(Xtr_all_fin, y_tr.loc[X_tr_allF.index])\n",
    "        proba_all = clf_all.predict_proba(Xva_all_fin)[:,1]\n",
    "    else:\n",
    "        catA, binA, numA = split_cols(X_tr_allF.columns)\n",
    "        pipe_all = Pipeline([(\"pre\", build_pre(catA, binA, numA)),\n",
    "                             (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C_LOGREG, class_weight=\"balanced\", max_iter=4000, random_state=RND))])\n",
    "        m_all = pipe_all.fit(X_tr_allF, y_tr.loc[X_tr_allF.index])\n",
    "        proba_all = m_all.predict_proba(X_te_allF)[:,1]\n",
    "\n",
    "    # Scores\n",
    "    hold_auc_best = roc_auc_score(y_true_best, proba_best)\n",
    "    hold_pr_best  = average_precision_score(y_true_best, proba_best)\n",
    "    hold_auc_all  = roc_auc_score(y_te.loc[X_te_allF.index], proba_all)\n",
    "    hold_pr_all   = average_precision_score(y_te.loc[X_te_allF.index], proba_all)\n",
    "\n",
    "    # --- Plot: Precision-Recall (best vs all_features)\n",
    "    prec_b, rec_b, _ = precision_recall_curve(y_true_best, proba_best)\n",
    "    prec_a, rec_a, _ = precision_recall_curve(y_te.loc[X_te_allF.index], proba_all)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(rec_b, prec_b, label=f\"Best (AP={hold_pr_best:.3f}, AUC={hold_auc_best:.3f})\")\n",
    "    plt.plot(rec_a, prec_a, label=f\"All-features (AP={hold_pr_all:.3f}, AUC={hold_auc_all:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Holdout Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(reports/\"holdout_pr_curve.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # Optional GBM smoke-test on best/all (using OHE pre)\n",
    "    gbm_out = None\n",
    "    if GBM_CHECK:\n",
    "        catB, binB, numB = split_cols(X_tr_best.columns); preB = build_pre(catB, binB, numB)\n",
    "        XtrB = preB.fit_transform(X_tr_best); XvaB = preB.transform(X_te_best)\n",
    "        gbm_best = holdout_gbm_check(XtrB, y_tr.loc[X_tr_best.index], XvaB, y_true_best)\n",
    "        catC, binC, numC = split_cols(X_tr_allF.columns); preC = build_pre(catC, binC, numC)\n",
    "        XtrC = preC.fit_transform(X_tr_allF); XvaC = preC.transform(X_te_allF)\n",
    "        gbm_all  = holdout_gbm_check(XtrC, y_tr.loc[X_tr_allF.index], XvaC, y_te.loc[X_te_allF.index])\n",
    "        gbm_out = {\"best\": gbm_best, \"all\": gbm_all}\n",
    "\n",
    "    # Meta\n",
    "    meta = {\n",
    "        \"random_state\": RND, \"cv_splits\": CV, \"C\": C_LOGREG,\n",
    "        \"n_rows_total\": int(n_rows_total), \"sample_n\": int(len(df)),\n",
    "        \"te_cat\": TE_CAT, \"te_alpha\": TE_ALPHA, \"gbm_check\": bool(GBM_CHECK),\n",
    "        \"scores_path\": str(res_path),\n",
    "        \"features_path\": str(reports/\"features_selected.csv\"),\n",
    "        \"split_indices_path\": str(reports/\"split_indices.json\"),\n",
    "        \"pr_curve_path\": str(reports/\"holdout_pr_curve.png\"),\n",
    "        \"best_by_cv\": best_cv,\n",
    "        \"holdout_scores\": {\n",
    "            \"best_auc\": float(hold_auc_best), \"best_pr_auc\": float(hold_pr_best),\n",
    "            \"all_auc\": float(hold_auc_all), \"all_pr_auc\": float(hold_pr_all)\n",
    "        },\n",
    "        \"gbm_holdout\": gbm_out\n",
    "    }\n",
    "    (reports/\"feature_gate_meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "    # Console\n",
    "    print(\"\\nFEATURE-GATE done.\")\n",
    "    print(f\"Train n={len(df_tr):,}, Holdout n={len(df_te):,}, CV={CV}, C={C_LOGREG}, TE_CAT={int(TE_CAT)}\")\n",
    "    print(\"Scores (CV):\\n\" + res.head(10).to_string(index=False))\n",
    "    print(f\"\\nHoldout (Best by CV): AUC={hold_auc_best:.4f}  PR-AUC={hold_pr_best:.4f}\")\n",
    "    print(f\"Holdout (All-features): AUC={hold_auc_all:.4f}  PR-AUC={hold_pr_all:.4f}\")\n",
    "    print(\"\\nArtifacts:\")\n",
    "    print(\"→ features_selected.csv\")\n",
    "    print(\"→ split_indices.json\")\n",
    "    print(\"→ holdout_pr_curve.png\")\n",
    "    print(\"(+ feature_gate_scores.csv, feature_gate_meta.json)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e895ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Using reports dir: /Users/lucasbeseler/ada_portoSeguro/reports\n",
      "Lade Datensatz aus dem Cache.\n",
      "OK: artifacts present\n",
      "OK: split disjoint, |train|=200,000, |test|=50,000, class drift=0.0000\n",
      "OK: 37 features loaded and reconstructed\n",
      "OK: minimal TE pipeline — Holdout AUC=0.631, PR-AUC=0.063\n",
      "OK: PR-curve file present and non-empty\n",
      "ALL TESTS PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Root resolution\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    CWD = Path.cwd()\n",
    "    ROOT = CWD.parent if CWD.name in (\"tests\", \"notebooks\") else CWD\n",
    "\n",
    "# ---- Reports dir (ENV override supported)\n",
    "RPT = Path(os.getenv(\"REPORTS_DIR\") or (ROOT / \"reports\"))\n",
    "print(f\"[TEST] Using reports dir: {RPT}\")\n",
    "\n",
    "# ---- Project loader\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "# ---- Sklearn bits\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# ---- Helpers\n",
    "def ohe_fallback():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"infrequent_if_exist\",\n",
    "                             min_frequency=0.01, sparse_output=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def split_cols(cols):\n",
    "    cols = list(cols)\n",
    "    cat = [c for c in cols if c.endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if c.endswith(\"_bin\")]\n",
    "    num  = [c for c in cols if (c not in cat and c not in bin_ and c != \"target\")]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def build_pre(cat, bin_, num):\n",
    "    cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_fallback())])\n",
    "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "    bin_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    return ColumnTransformer([(\"cat\", cat_pipe, cat),\n",
    "                              (\"bin\", bin_pipe, bin_),\n",
    "                              (\"num\", num_pipe, num)], remainder=\"drop\")\n",
    "\n",
    "# ---- Minimal TE utils (self-contained)\n",
    "def _prep_te_blocks(X_tr, X_te):\n",
    "    cat, bin_, num = split_cols(X_tr.columns)\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_bin = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def fit_transform_block(imp, A, cols):\n",
    "        if not cols: return pd.DataFrame(np.empty((len(A),0)), index=A.index)\n",
    "        return pd.DataFrame(imp.fit_transform(A[cols]), columns=cols, index=A.index)\n",
    "\n",
    "    def transform_block(imp, A, cols):\n",
    "        if not cols: return pd.DataFrame(np.empty((len(A),0)), index=A.index)\n",
    "        return pd.DataFrame(imp.transform(A[cols]), columns=cols, index=A.index)\n",
    "\n",
    "    Xtr_cat = fit_transform_block(imp_cat, X_tr, cat)\n",
    "    Xte_cat = transform_block(imp_cat, X_te, cat)\n",
    "    Xtr_bin = fit_transform_block(imp_bin, X_tr, bin_)\n",
    "    Xte_bin = transform_block(imp_bin, X_te, bin_)\n",
    "    Xtr_num = fit_transform_block(imp_num, X_tr, num)\n",
    "    Xte_num = transform_block(imp_num, X_te, num)\n",
    "\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    if Xtr_num.shape[1]:\n",
    "        Xtr_num = pd.DataFrame(sc.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "        Xte_num = pd.DataFrame(sc.transform(Xte_num), columns=Xte_num.columns, index=Xte_num.index)\n",
    "\n",
    "    return (Xtr_cat, Xte_cat, Xtr_bin, Xte_bin, Xtr_num, Xte_num)\n",
    "\n",
    "def _kfold_target_encode(train_cat, y_tr, valid_cat, n_splits=3, alpha=10, seed=42):\n",
    "    if train_cat.shape[1] == 0:\n",
    "        return pd.DataFrame(index=train_cat.index), pd.DataFrame(index=valid_cat.index)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = y_tr.mean()\n",
    "    tr_enc = pd.DataFrame(index=train_cat.index)\n",
    "    va_enc = pd.DataFrame(index=valid_cat.index)\n",
    "    for col in train_cat.columns:\n",
    "        oof = pd.Series(index=train_cat.index, dtype=float)\n",
    "        for tr_idx, va_idx in skf.split(train_cat, y_tr):\n",
    "            col_tr = train_cat.iloc[tr_idx][col]\n",
    "            y_sub  = y_tr.iloc[tr_idx]\n",
    "            stats = y_sub.groupby(col_tr).agg(['mean','count'])\n",
    "            m = (stats['mean']*stats['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "            oof.iloc[va_idx] = train_cat.iloc[va_idx][col].map(m)\n",
    "        tr_enc[col] = oof.fillna(global_mean)\n",
    "        stats_full = y_tr.groupby(train_cat[col]).agg(['mean','count'])\n",
    "        m_full = (stats_full['mean']*stats_full['count'] + global_mean*alpha) / (stats_full['count'] + alpha)\n",
    "        va_enc[col] = valid_cat[col].map(m_full).fillna(global_mean)\n",
    "    tr_enc.columns = [f\"te_{c}\" for c in tr_enc.columns]\n",
    "    va_enc.columns = [f\"te_{c}\" for c in va_enc.columns]\n",
    "    return tr_enc, va_enc\n",
    "\n",
    "# ---- Feature reconstruction (same as gate)\n",
    "def fe_simple(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    b = [c for c in X.columns if c.endswith(\"_bin\")]\n",
    "    if b:\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1)\n",
    "    return X\n",
    "\n",
    "def make_feature_set(df, drop_calc=True, extra_drop=None, add_extras=True):\n",
    "    X = df.drop(columns=[\"target\"], errors=\"ignore\").copy().replace(-1, np.nan)\n",
    "    if drop_calc:\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\"ps_calc_\")], errors=\"ignore\")\n",
    "    if extra_drop:\n",
    "        X = X.drop(columns=[c for c in extra_drop if c in X.columns], errors=\"ignore\")\n",
    "    if add_extras:\n",
    "        X = fe_simple(X)\n",
    "    return X\n",
    "\n",
    "def main():\n",
    "    # --- Artifacts existence\n",
    "    paths = {\n",
    "        \"features\": RPT / \"features_selected.csv\",\n",
    "        \"split\":    RPT / \"split_indices.json\",\n",
    "        \"meta\":     RPT / \"feature_gate_meta.json\",\n",
    "        \"pr_png\":   RPT / \"holdout_pr_curve.png\",\n",
    "        \"scores\":   RPT / \"feature_gate_scores.csv\",\n",
    "    }\n",
    "    for k,p in paths.items():\n",
    "        assert p.exists(), f\"{k} artifact missing: {p}\"\n",
    "\n",
    "    # --- Load artifacts\n",
    "    feats = pd.read_csv(paths[\"features\"])[\"raw_feature\"].tolist()\n",
    "    assert len(feats) > 0, \"features_selected.csv is empty\"\n",
    "\n",
    "    with open(paths[\"split\"], \"r\") as f: split = json.load(f)\n",
    "    with open(paths[\"meta\"], \"r\") as f: meta = json.load(f)\n",
    "\n",
    "    train_idx = split[\"train\"]; test_idx = split[\"test\"]\n",
    "    assert len(train_idx) > 0 and len(test_idx) > 0, \"split indices are empty\"\n",
    "    assert set(train_idx).isdisjoint(set(test_idx)), \"train/test indices overlap!\"\n",
    "    assert paths[\"pr_png\"].stat().st_size > 1000, \"holdout_pr_curve.png too small or empty\"\n",
    "\n",
    "    # --- Load data, align indices\n",
    "    df_all = load_and_save_data().replace(-1, np.nan)\n",
    "    miss_tr = [i for i in train_idx if i not in df_all.index]\n",
    "    miss_te = [i for i in test_idx  if i not in df_all.index]\n",
    "    assert not miss_tr and not miss_te, \"Saved indices not in current dataframe index\"\n",
    "\n",
    "    df_tr = df_all.loc[train_idx]\n",
    "    df_te = df_all.loc[test_idx]\n",
    "    assert \"target\" in df_tr.columns, \"target column missing\"\n",
    "\n",
    "    y_tr = df_tr[\"target\"].astype(int)\n",
    "    y_te = df_te[\"target\"].astype(int)\n",
    "\n",
    "    # --- Rebuild features using SAME recipe as gate\n",
    "    best = meta.get(\"best_by_cv\", {})\n",
    "    drop_calc  = bool(best.get(\"drop_calc\", True))\n",
    "    extra_drop = best.get(\"extra_drop\", []) or []\n",
    "    add_extras = bool(best.get(\"add_extras\", False))\n",
    "\n",
    "    X_tr_full = make_feature_set(df_tr, drop_calc=drop_calc, extra_drop=extra_drop, add_extras=add_extras)\n",
    "    X_te_full = make_feature_set(df_te, drop_calc=drop_calc, extra_drop=extra_drop, add_extras=add_extras)\n",
    "\n",
    "    missing_cols = [c for c in feats if c not in X_tr_full.columns]\n",
    "    assert not missing_cols, f\"Some saved features not in reconstructed feature set: {missing_cols[:5]}\"\n",
    "\n",
    "    X_tr = X_tr_full[feats].copy()\n",
    "    X_te = X_te_full[feats].copy()\n",
    "\n",
    "    # --- Sanity on stratification\n",
    "    diff_rate = abs(y_tr.mean() - y_te.mean())\n",
    "    assert diff_rate < 0.01, f\"Class rate drift too high between splits: {diff_rate:.4f}\"\n",
    "\n",
    "    # --- Minimal train run (mirrors meta)\n",
    "    te_cat   = bool(meta.get(\"te_cat\", False))\n",
    "    rnd      = int(meta.get(\"random_state\", 42))\n",
    "    C        = float(meta.get(\"C\", 1.0))\n",
    "    cv       = int(meta.get(\"cv_splits\", 3))\n",
    "    te_alpha = float(meta.get(\"te_alpha\", 10))\n",
    "\n",
    "    if te_cat:\n",
    "        # TE path\n",
    "        Xtr_cat, Xte_cat, Xtr_bin, Xte_bin, Xtr_num, Xte_num = _prep_te_blocks(X_tr, X_te)\n",
    "        tr_te, te_te = _kfold_target_encode(Xtr_cat, y_tr, Xte_cat, n_splits=cv, alpha=te_alpha, seed=rnd)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xte_fin = pd.concat([Xte_num, Xte_bin, te_te], axis=1)\n",
    "        clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C,\n",
    "                                 class_weight=\"balanced\", max_iter=4000, random_state=rnd)\n",
    "        clf.fit(Xtr_fin, y_tr)\n",
    "        proba = clf.predict_proba(Xte_fin)[:, 1]\n",
    "    else:\n",
    "        # OHE path\n",
    "        cat, bin_, num = split_cols(X_tr.columns)\n",
    "        pre  = build_pre(cat, bin_, num)\n",
    "        clf  = LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C,\n",
    "                                  class_weight=\"balanced\", max_iter=4000, random_state=rnd)\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        proba = pipe.predict_proba(X_te)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_te, proba)\n",
    "    ap  = average_precision_score(y_te, proba)\n",
    "\n",
    "    # --- Soft sanity ranges (dataset-specific)\n",
    "    assert 0.5 <= auc <= 0.9, f\"AUC out of expected sanity range: {auc:.3f}\"\n",
    "    assert 0.02 <= ap <= 0.5, f\"PR-AUC out of expected sanity range: {ap:.3f}\"\n",
    "\n",
    "    # --- OK prints\n",
    "    print(\"OK: artifacts present\")\n",
    "    print(f\"OK: split disjoint, |train|={len(train_idx):,}, |test|={len(test_idx):,}, class drift={diff_rate:.4f}\")\n",
    "    print(f\"OK: {len(feats)} features loaded and reconstructed\")\n",
    "    print(f\"OK: minimal {'TE' if te_cat else 'OHE'} pipeline — Holdout AUC={auc:.3f}, PR-AUC={ap:.3f}\")\n",
    "    print(\"OK: PR-curve file present and non-empty\")\n",
    "    print(\"ALL TESTS PASSED ✅\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except AssertionError as e:\n",
    "        print(f\"TEST FAILED ❌  {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as ex:\n",
    "        print(f\"TEST ERROR ❌  {type(ex).__name__}: {ex}\")\n",
    "        sys.exit(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7872bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ABLATION] Using reports dir: /Users/lucasbeseler/ada_portoSeguro/reports\n",
      "Lade Datensatz aus dem Cache.\n",
      "[ABLATION] Saved: /Users/lucasbeseler/ada_portoSeguro/reports/feature_ablation_results.csv\n",
      "[ABLATION] Plot saved: /Users/lucasbeseler/ada_portoSeguro/reports/feature_ablation_auc.png\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------- Root + reports dir ----------\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    CWD = Path.cwd()\n",
    "    ROOT = CWD.parent if CWD.name in (\"tests\", \"notebooks\", \"tools\") else CWD\n",
    "\n",
    "RPT = Path(os.getenv(\"REPORTS_DIR\") or (ROOT / \"reports\"))\n",
    "if not RPT.exists():\n",
    "    alt = ROOT.parent / \"reports\"\n",
    "    if alt.exists():\n",
    "        RPT = alt\n",
    "print(f\"[ABLATION] Using reports dir: {RPT}\")\n",
    "\n",
    "# ---------- Project loader ----------\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "# ---------- Sklearn ----------\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------- Plot (matplotlib only) ----------\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def ohe_fallback():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"infrequent_if_exist\",\n",
    "                             min_frequency=0.01, sparse_output=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def split_cols(cols):\n",
    "    cols = list(cols)\n",
    "    cat = [c for c in cols if c.endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if c.endswith(\"_bin\")]\n",
    "    num  = [c for c in cols if (c not in cat and c not in bin_ and c != \"target\")]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def build_pre(cat, bin_, num):\n",
    "    cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_fallback())])\n",
    "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "    bin_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    return ColumnTransformer([(\"cat\", cat_pipe, cat),\n",
    "                              (\"bin\", bin_pipe, bin_),\n",
    "                              (\"num\", num_pipe, num)], remainder=\"drop\")\n",
    "\n",
    "def fe_simple(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    b = [c for c in X.columns if c.endswith(\"_bin\")]\n",
    "    if b:\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1)\n",
    "    return X\n",
    "\n",
    "def make_feature_set(df, drop_calc=True, extra_drop=None, add_extras=True):\n",
    "    X = df.drop(columns=[\"target\"], errors=\"ignore\").copy().replace(-1, np.nan)\n",
    "    if drop_calc:\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\"ps_calc_\")], errors=\"ignore\")\n",
    "    if extra_drop:\n",
    "        X = X.drop(columns=[c for c in extra_drop if c in X.columns], errors=\"ignore\")\n",
    "    if add_extras:\n",
    "        X = fe_simple(X)\n",
    "    return X\n",
    "\n",
    "# ---------- TE utilities ----------\n",
    "def _prep_te_blocks(X_tr, X_va):\n",
    "    cat, bin_, num = split_cols(X_tr.columns)\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_bin = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    def fit_transform_block(imp, A, cols):\n",
    "        if not cols: return pd.DataFrame(np.empty((len(A),0)), index=A.index)\n",
    "        return pd.DataFrame(imp.fit_transform(A[cols]), columns=cols, index=A.index)\n",
    "\n",
    "    def transform_block(imp, A, cols):\n",
    "        if not cols: return pd.DataFrame(np.empty((len(A),0)), index=A.index)\n",
    "        return pd.DataFrame(imp.transform(A[cols]), columns=cols, index=A.index)\n",
    "\n",
    "    Xtr_cat = fit_transform_block(imp_cat, X_tr, cat)\n",
    "    Xva_cat = transform_block(imp_cat, X_va, cat)\n",
    "    Xtr_bin = fit_transform_block(imp_bin, X_tr, bin_)\n",
    "    Xva_bin = transform_block(imp_bin, X_va, bin_)\n",
    "    Xtr_num = fit_transform_block(imp_num, X_tr, num)\n",
    "    Xva_num = transform_block(imp_num, X_va, num)\n",
    "\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    if Xtr_num.shape[1]:\n",
    "        Xtr_num = pd.DataFrame(sc.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "        Xva_num = pd.DataFrame(sc.transform(Xva_num), columns=Xva_num.columns, index=Xva_num.index)\n",
    "\n",
    "    return (Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num)\n",
    "\n",
    "def _kfold_target_encode(train_cat, y_tr, valid_cat, n_splits=3, alpha=10, seed=42):\n",
    "    if train_cat.shape[1] == 0:\n",
    "        return pd.DataFrame(index=train_cat.index), pd.DataFrame(index=valid_cat.index)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = y_tr.mean()\n",
    "    tr_enc = pd.DataFrame(index=train_cat.index)\n",
    "    va_enc = pd.DataFrame(index=valid_cat.index)\n",
    "    for col in train_cat.columns:\n",
    "        oof = pd.Series(index=train_cat.index, dtype=float)\n",
    "        for tr_idx, va_idx in skf.split(train_cat, y_tr):\n",
    "            col_tr = train_cat.iloc[tr_idx][col]; y_sub = y_tr.iloc[tr_idx]\n",
    "            stats = y_sub.groupby(col_tr).agg(['mean','count'])\n",
    "            m = (stats['mean']*stats['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "            oof.iloc[va_idx] = train_cat.iloc[va_idx][col].map(m)\n",
    "        tr_enc[col] = oof.fillna(global_mean)\n",
    "        stats_full = y_tr.groupby(train_cat[col]).agg(['mean','count'])\n",
    "        m_full = (stats_full['mean']*stats_full['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "        va_enc[col] = valid_cat[col].map(m_full).fillna(global_mean)\n",
    "    tr_enc.columns = [f\"te_{c}\" for c in tr_enc.columns]\n",
    "    va_enc.columns = [f\"te_{c}\" for c in va_enc.columns]\n",
    "    return tr_enc, va_enc\n",
    "\n",
    "# ---------- CV runners ----------\n",
    "def cv_mean_auc_OHE(X, y, seed=42, cv=3, C=1.0):\n",
    "    cat, bin_, num = split_cols(X.columns)\n",
    "    pre  = build_pre(cat, bin_, num)\n",
    "    clf  = LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C,\n",
    "                              class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "    aucs = []\n",
    "    for tr, va in skf.split(X, y):\n",
    "        m = pipe.fit(X.iloc[tr], y.iloc[tr])\n",
    "        p = m.predict_proba(X.iloc[va])[:,1]\n",
    "        aucs.append(roc_auc_score(y.iloc[va], p))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "def cv_mean_auc_TE(X, y, seed=42, cv=3, C=1.0, alpha=10):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed)\n",
    "    aucs = []\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr, X_va)\n",
    "        tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr, Xva_cat, n_splits=cv, alpha=alpha, seed=seed)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C,\n",
    "                                 class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "        clf.fit(Xtr_fin, y_tr)\n",
    "        p = clf.predict_proba(Xva_fin)[:,1]\n",
    "        aucs.append(roc_auc_score(y_va, p))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "def bootstrap_ci(values, n_boot=2000, alpha=0.05, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    vals = np.array(values, dtype=float)\n",
    "    if len(vals) == 0:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        samp = rng.choice(vals, size=len(vals), replace=True)\n",
    "        boots.append(np.mean(samp))\n",
    "    lo = np.percentile(boots, 100*alpha/2)\n",
    "    hi = np.percentile(boots, 100*(1 - alpha/2))\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def main():\n",
    "    # ----- artifacts -----\n",
    "    meta_p  = RPT / \"feature_gate_meta.json\"\n",
    "    split_p = RPT / \"split_indices.json\"\n",
    "    assert meta_p.exists() and split_p.exists(), \"Missing meta/split artifacts\"\n",
    "    meta  = json.loads(meta_p.read_text())\n",
    "    split = json.loads(split_p.read_text())\n",
    "\n",
    "    # ----- data + split -----\n",
    "    df = load_and_save_data().replace(-1, np.nan)\n",
    "    df_tr = df.loc[split[\"train\"]].copy()\n",
    "    y = df_tr[\"target\"].astype(int)\n",
    "\n",
    "    # ----- base recipe -----\n",
    "    best = meta[\"best_by_cv\"]\n",
    "    drop_calc  = bool(best.get(\"drop_calc\", True))\n",
    "    extra_drop = best.get(\"extra_drop\", [])\n",
    "    if isinstance(extra_drop, str):\n",
    "        try: extra_drop = list(eval(extra_drop))\n",
    "        except Exception: extra_drop = []\n",
    "    add_extras = bool(best.get(\"add_extras\", False))\n",
    "\n",
    "    X_base_full = make_feature_set(df_tr, drop_calc=drop_calc, extra_drop=extra_drop, add_extras=add_extras)\n",
    "    base_cols = list(X_base_full.columns)\n",
    "\n",
    "    # candidates\n",
    "    add_candidates  = [f for f in extra_drop if f in df_tr.columns]  # previously dropped by config\n",
    "    drop_candidates = base_cols[:]                                   # currently kept\n",
    "\n",
    "    te_cat   = bool(meta.get(\"te_cat\", False))\n",
    "    cv       = int(meta.get(\"cv_splits\", 3))\n",
    "    C        = float(meta.get(\"C\", 1.0))\n",
    "    te_alpha = float(meta.get(\"te_alpha\", 10))\n",
    "    seeds    = [42,43,44,45,46]  # multi-seed for stability\n",
    "\n",
    "    def cv_mean_auc(X, y, seed):\n",
    "        return cv_mean_auc_TE(X, y, seed=seed, cv=cv, C=C, alpha=te_alpha) if te_cat else \\\n",
    "               cv_mean_auc_OHE(X, y, seed=seed, cv=cv, C=C)\n",
    "\n",
    "    # baseline (once per seed)\n",
    "    base_aucs = {sd: cv_mean_auc(X_base_full, y, seed=sd) for sd in seeds}\n",
    "\n",
    "    # predefine columns so empty frames don't KeyError\n",
    "    cols = [\"feature\",\"action\",\"n_features_variant\",\n",
    "            \"auc_base_mean\",\"auc_variant_mean\",\n",
    "            \"delta_auc_mean\",\"delta_auc_ci_low\",\"delta_auc_ci_high\",\n",
    "            \"seeds\",\"cv\"]\n",
    "    rows = []\n",
    "\n",
    "    # DROP pass\n",
    "    for f in drop_candidates:\n",
    "        X_var = X_base_full.drop(columns=[f], errors=\"ignore\")\n",
    "        auc_deltas, aucs_var = [], []\n",
    "        for sd in seeds:\n",
    "            auc_b = base_aucs[sd]\n",
    "            auc_v = cv_mean_auc(X_var, y, seed=sd)\n",
    "            aucs_var.append(auc_v)\n",
    "            auc_deltas.append(auc_v - auc_b)\n",
    "        lo, hi = bootstrap_ci(auc_deltas, n_boot=2000, alpha=0.05, seed=seeds[0])\n",
    "        rows.append([f, \"DROP\", int(X_var.shape[1]),\n",
    "                     float(np.mean(list(base_aucs.values()))), float(np.mean(aucs_var)),\n",
    "                     float(np.mean(auc_deltas)), lo, hi,\n",
    "                     \",\".join(map(str, seeds)), cv])\n",
    "\n",
    "    # ADD pass\n",
    "    for f in add_candidates:\n",
    "        extra_drop_new = [x for x in extra_drop if x != f]\n",
    "        X_var_full = make_feature_set(df_tr, drop_calc=drop_calc, extra_drop=extra_drop_new, add_extras=add_extras)\n",
    "        if f not in X_var_full.columns:\n",
    "            continue\n",
    "        auc_deltas, aucs_var = [], []\n",
    "        for sd in seeds:\n",
    "            auc_b = base_aucs[sd]\n",
    "            auc_v = cv_mean_auc(X_var_full, y, seed=sd)\n",
    "            aucs_var.append(auc_v)\n",
    "            auc_deltas.append(auc_v - auc_b)\n",
    "        lo, hi = bootstrap_ci(auc_deltas, n_boot=2000, alpha=0.05, seed=seeds[0])\n",
    "        rows.append([f, \"ADD\", int(X_var_full.shape[1]),\n",
    "                     float(np.mean(list(base_aucs.values()))), float(np.mean(aucs_var)),\n",
    "                     float(np.mean(auc_deltas)), lo, hi,\n",
    "                     \",\".join(map(str, seeds)), cv])\n",
    "\n",
    "    # results frame (safe even if empty)\n",
    "    res = pd.DataFrame(rows, columns=cols)\n",
    "    out_csv = RPT / \"feature_ablation_results.csv\"\n",
    "\n",
    "    if res.empty:\n",
    "        # write header-only CSV and exit gracefully\n",
    "        res.to_csv(out_csv, index=False)\n",
    "        print(f\"[ABLATION] No candidates found. Wrote header to: {out_csv}\")\n",
    "        return\n",
    "\n",
    "    # sort + save\n",
    "    res[\"abs_delta\"] = res[\"delta_auc_mean\"].abs()\n",
    "    res_sorted = res.sort_values([\"action\",\"abs_delta\"], ascending=[True, False])\n",
    "    res_sorted.to_csv(out_csv, index=False)\n",
    "    print(f\"[ABLATION] Saved: {out_csv}\")\n",
    "\n",
    "    # plot top-|ΔAUC| with 95% CI\n",
    "    topk = min(25, len(res_sorted))\n",
    "    sub = res_sorted.sort_values(\"abs_delta\", ascending=False).head(topk)\n",
    "\n",
    "    xlbl = sub.apply(lambda r: f'{r[\"action\"]}:{r[\"feature\"]}', axis=1)\n",
    "    ymu  = sub[\"delta_auc_mean\"].values\n",
    "    yerr = np.vstack([\n",
    "        ymu - sub[\"delta_auc_ci_low\"].values,\n",
    "        sub[\"delta_auc_ci_high\"].values - ymu\n",
    "    ])\n",
    "\n",
    "    plt.figure(figsize=(10, max(4, 0.35*len(sub))))\n",
    "    plt.errorbar(ymu, np.arange(len(sub)), xerr=yerr, fmt='o', capsize=3)\n",
    "    plt.yticks(np.arange(len(sub)), xlbl)\n",
    "    plt.axvline(0.0, linestyle=\"--\")\n",
    "    plt.xlabel(\"ΔAUC (variant - base) with 95% CI\")\n",
    "    plt.title(f\"Feature ablation (seeds={len(seeds)}, cv={cv}) — Top {len(sub)} by |ΔAUC|\")\n",
    "    plt.tight_layout()\n",
    "    out_png = RPT / \"feature_ablation_auc.png\"\n",
    "    plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[ABLATION] Plot saved: {out_png}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except AssertionError as e:\n",
    "        print(f\"ABLATION FAILED ❌  {e}\")\n",
    "        sys.exit(1)\n",
    "    except Exception as ex:\n",
    "        print(f\"ABLATION ERROR ❌  {type(ex).__name__}: {ex}\")\n",
    "        sys.exit(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e184f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/lucasbeseler/ada_portoSeguro/reports/feature_ablation_decisions.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ENV oder globalen reports-Pfad verwenden\n",
    "RPT = Path(os.getenv(\"REPORTS_DIR\") or \"/Users/lucasbeseler/ada_portoSeguro/reports\")\n",
    "df = pd.read_csv(RPT / \"feature_ablation_results.csv\")\n",
    "\n",
    "def decide(row):\n",
    "    if row[\"action\"] == \"DROP\":\n",
    "        if row[\"delta_auc_ci_high\"] < 0:  # CI < 0: Droppen schadet\n",
    "            return \"KEEP\"\n",
    "        if row[\"delta_auc_ci_low\"] > 0:   # CI > 0: Droppen hilft\n",
    "            return \"REMOVE\"\n",
    "        return \"NEUTRAL\"\n",
    "    if row[\"action\"] == \"ADD\":\n",
    "        if row[\"delta_auc_ci_low\"] > 0:   # Hinzufügen hilft\n",
    "            return \"ADD\"\n",
    "        if row[\"delta_auc_ci_high\"] < 0:  # Hinzufügen schadet\n",
    "            return \"DONT_ADD\"\n",
    "        return \"NEUTRAL\"\n",
    "    return \"NEUTRAL\"\n",
    "\n",
    "out = df.copy()\n",
    "out[\"decision\"] = out.apply(decide, axis=1)\n",
    "out.to_csv(RPT / \"feature_ablation_decisions.csv\", index=False)\n",
    "print(\"Wrote:\", RPT / \"feature_ablation_decisions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6778c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
