{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Datensatz aus dem Cache.\n",
      "\n",
      "FEATURE-GATE fertig.\n",
      "Train n=200,000, Holdout n=50,000, CV=3, C=1.0, TE_CAT=0, GBM_CHECK=0\n",
      "Scores (CV):\n",
      "                name  n_features   cv_auc  cv_pr_auc  drop_calc                 extra_drop  add_extras  te_cat\n",
      "drop_calc+opt+extras          37 0.620888   0.059965       True [ps_ind_14, ps_car_10_cat]        True   False\n",
      "    drop_calc+extras          39 0.620864   0.059968       True                         []        True   False\n",
      "      drop_calc_only          37 0.620800   0.059986       True                         []       False   False\n",
      "        all_features          57 0.619775   0.059835      False                         []       False   False\n",
      "\n",
      "Holdout drop_calc+opt+extras:   AUC=0.6325  PR-AUC=0.0651\n",
      "Holdout all_features: AUC=0.6284  PR-AUC=0.0645\n",
      "\n",
      "Gewählt: drop_calc+opt+extras\n",
      "→ Spalten: /Users/lucasbeseler/ada_portoSeguro/reports/features_selected.csv\n",
      "→ Scores:  /Users/lucasbeseler/ada_portoSeguro/reports/feature_gate_scores.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Fast Feature-Gate zwischen EDA und ML\n",
    "# Optional zuschaltbar: Target Encoding (TE) & LightGBM-Check. Standard: AUS (schnell).\n",
    "\n",
    "import os, sys, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Repo-ROOT robust (Notebook/Skript)\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    CWD = Path.cwd(); ROOT = CWD.parent if CWD.name == \"notebooks\" else CWD\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Einstellungen\n",
    "if 'get_ipython' in globals():\n",
    "    os.environ.setdefault(\"CV\", \"3\")\n",
    "    os.environ.setdefault(\"RND\", \"42\")\n",
    "    os.environ.setdefault(\"TE_CAT\", \"1\")\n",
    "    os.environ.setdefault(\"GBM_CHECK\", \"1\")\n",
    "    os.environ.setdefault(\"TRAIN_SAMPLE_N\", \"250000\")\n",
    "\n",
    "# --- Runtime-Parameter (per ENV steuerbar)\n",
    "RND = int(os.getenv(\"RND\", \"42\"))\n",
    "\n",
    "# --- Helpers\n",
    "\n",
    "def ohe_fallback():\n",
    "    \"\"\"Sklearn-kompatibles OHE mit Fallbacks für ältere Versionen.\"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"infrequent_if_exist\", min_frequency=0.01, sparse_output=True)\n",
    "    except TypeError:\n",
    "        try:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "        except TypeError:\n",
    "            return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def split_cols(cols):\n",
    "    cat = [c for c in cols if c.endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if c.endswith(\"_bin\")]\n",
    "    num  = [c for c in cols if (c not in cat and c not in bin_ and c != \"target\")]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def fe_simple(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    b = [c for c in X.columns if c.endswith(\"_bin\")]\n",
    "    if b:\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1)\n",
    "    return X\n",
    "\n",
    "def build_pre(cat, bin_, num):\n",
    "    cat_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")), (\"ohe\", ohe_fallback())])\n",
    "    num_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())])\n",
    "    bin_pipe = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\"))])\n",
    "    return ColumnTransformer([(\"cat\", cat_pipe, cat), (\"bin\", bin_pipe, bin_), (\"num\", num_pipe, num)], remainder=\"drop\")\n",
    "\n",
    "def make_feature_set(df, drop_calc=True, extra_drop=None, add_extras=True, drop_groups=None):\n",
    "    X = df.drop(columns=[\"target\"], errors=\"ignore\").copy().replace(-1, np.nan)\n",
    "    if drop_calc:\n",
    "        X = X.drop(columns=[c for c in X.columns if c.startswith(\"ps_calc_\")], errors=\"ignore\")\n",
    "    if extra_drop:\n",
    "        X = X.drop(columns=[c for c in extra_drop if c in X.columns], errors=\"ignore\")\n",
    "    extras_cols = []\n",
    "    if add_extras:\n",
    "        X = fe_simple(X); extras_cols = [\"missing_count\", \"sum_all_bin\"]\n",
    "    if drop_groups:\n",
    "        cat, bin_, num = split_cols(X.columns)\n",
    "        if drop_groups.get(\"cat\"):   X = X.drop(columns=cat, errors=\"ignore\")\n",
    "        if drop_groups.get(\"bin\"):   X = X.drop(columns=bin_, errors=\"ignore\")\n",
    "        if drop_groups.get(\"num\"):   X = X.drop(columns=num, errors=\"ignore\")\n",
    "        if drop_groups.get(\"extras\"): X = X.drop(columns=[c for c in extras_cols if c in X.columns], errors=\"ignore\")\n",
    "    return X\n",
    "\n",
    "def cv_scores_ohe(X, y, C=1.0, CV=3, seed=RND):\n",
    "    \"\"\"OOF-CV Scores mit OHE + LogReg (schnell, baseline).\"\"\"\n",
    "    cat, bin_, num = split_cols(X.columns)\n",
    "    pre  = build_pre(cat, bin_, num)\n",
    "    clf  = LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for tr, te in skf.split(X, y):\n",
    "        m = pipe.fit(X.iloc[tr], y.iloc[tr])\n",
    "        proba[te] = m.predict_proba(X.iloc[te])[:, 1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "# --- Target Encoding (TE) – optional\n",
    "\n",
    "def _kfold_target_encode(train_cat: pd.DataFrame, y_tr: pd.Series, valid_cat: pd.DataFrame, n_splits=3, alpha=10, seed=RND):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = y_tr.mean()\n",
    "    tr_enc = pd.DataFrame(index=train_cat.index)\n",
    "    va_enc = pd.DataFrame(index=valid_cat.index)\n",
    "    for col in train_cat.columns:\n",
    "        oof = pd.Series(index=train_cat.index, dtype=float)\n",
    "        for tr_idx, va_idx in skf.split(train_cat, y_tr):\n",
    "            col_tr = train_cat.iloc[tr_idx][col]\n",
    "            y_sub  = y_tr.iloc[tr_idx]\n",
    "            stats = y_sub.groupby(col_tr).agg(['mean','count'])\n",
    "            m = (stats['mean']*stats['count'] + global_mean*alpha) / (stats['count'] + alpha)\n",
    "            oof.iloc[va_idx] = train_cat.iloc[va_idx][col].map(m)\n",
    "        tr_enc[col] = oof.fillna(global_mean)\n",
    "        stats_full = y_tr.groupby(train_cat[col]).agg(['mean','count'])\n",
    "        m_full = (stats_full['mean']*stats_full['count'] + global_mean*alpha) / (stats_full['count'] + alpha)\n",
    "        va_enc[col] = valid_cat[col].map(m_full).fillna(global_mean)\n",
    "    tr_enc.columns = [f\"te_{c}\" for c in tr_enc.columns]\n",
    "    va_enc.columns = [f\"te_{c}\" for c in va_enc.columns]\n",
    "    return tr_enc, va_enc\n",
    "\n",
    "def _prep_te_blocks(X_tr, X_va):\n",
    "    cat, bin_, num = split_cols(X_tr.columns)\n",
    "    imp_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_bin = SimpleImputer(strategy=\"most_frequent\")\n",
    "    imp_num = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_cat = pd.DataFrame(imp_cat.fit_transform(X_tr[cat]) if cat else np.empty((len(X_tr),0)), columns=cat, index=X_tr.index)\n",
    "    Xva_cat = pd.DataFrame(imp_cat.transform(X_va[cat]) if cat else np.empty((len(X_va),0)), columns=cat, index=X_va.index)\n",
    "    Xtr_bin = pd.DataFrame(imp_bin.fit_transform(X_tr[bin_]) if bin_ else np.empty((len(X_tr),0)), columns=bin_, index=X_tr.index)\n",
    "    Xva_bin = pd.DataFrame(imp_bin.transform(X_va[bin_]) if bin_ else np.empty((len(X_va),0)), columns=bin_, index=X_va.index)\n",
    "    Xtr_num = pd.DataFrame(imp_num.fit_transform(X_tr[num]) if num else np.empty((len(X_tr),0)), columns=num, index=X_tr.index)\n",
    "    Xva_num = pd.DataFrame(imp_num.transform(X_va[num]) if num else np.empty((len(X_va),0)), columns=num, index=X_va.index)\n",
    "    sc = StandardScaler(with_mean=True, with_std=True)\n",
    "    if Xtr_num.shape[1]:\n",
    "        Xtr_num = pd.DataFrame(sc.fit_transform(Xtr_num), columns=Xtr_num.columns, index=Xtr_num.index)\n",
    "        Xva_num = pd.DataFrame(sc.transform(Xva_num), columns=Xva_num.columns, index=Xva_num.index)\n",
    "    return (Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num)\n",
    "\n",
    "def cv_scores_te(X, y, C=1.0, CV=3, seed=RND, alpha=10):\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=seed)\n",
    "    proba = np.zeros(len(y), dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(X, y):\n",
    "        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr, X_va)\n",
    "        if Xtr_cat.shape[1]:\n",
    "            tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr, Xva_cat, n_splits=CV, alpha=alpha, seed=seed)\n",
    "        else:\n",
    "            tr_te = pd.DataFrame(index=X_tr.index); va_te = pd.DataFrame(index=X_va.index)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=seed)\n",
    "        clf.fit(Xtr_fin, y_tr)\n",
    "        proba[va_idx] = clf.predict_proba(Xva_fin)[:,1]\n",
    "    return roc_auc_score(y, proba), average_precision_score(y, proba)\n",
    "\n",
    "# --- Optional: schneller GBM-Check (LightGBM)\n",
    "\n",
    "def holdout_gbm_check(X_tr, y_tr, X_te, y_te, seed=RND):\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "    except Exception:\n",
    "        print(\"[GBM] LightGBM nicht verfügbar – überspringe.\")\n",
    "        return None\n",
    "    clf = LGBMClassifier(n_estimators=300, learning_rate=0.1, num_leaves=31,\n",
    "                         subsample=0.8, colsample_bytree=0.8, reg_lambda=0.0,\n",
    "                         random_state=seed, n_jobs=-1)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    p = clf.predict_proba(X_te)[:,1]\n",
    "    return float(roc_auc_score(y_te, p)), float(average_precision_score(y_te, p))\n",
    "\n",
    "# --- Main\n",
    "\n",
    "def main():\n",
    "    CV = int(os.getenv(\"CV\", \"3\"))\n",
    "    C  = float(os.getenv(\"C\", \"1.0\"))\n",
    "    N  = int(os.getenv(\"TRAIN_SAMPLE_N\", \"250000\"))  # 0 = alles\n",
    "    TE_CAT = int(os.getenv(\"TE_CAT\", \"0\")) == 1\n",
    "    GBM_CHECK = int(os.getenv(\"GBM_CHECK\", \"0\")) == 1\n",
    "\n",
    "    reports = ROOT/\"reports\"; reports.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = load_and_save_data().replace(-1, np.nan)\n",
    "    if N and N < len(df):\n",
    "        df = df.sample(N, random_state=RND).sort_index()\n",
    "    y  = df[\"target\"].astype(int)\n",
    "\n",
    "    # Holdout\n",
    "    X_tr_all, X_te_all, y_tr, y_te = train_test_split(df.drop(columns=[\"target\"]), y, test_size=0.2, stratify=y, random_state=RND)\n",
    "    df_tr = pd.concat([X_tr_all, y_tr], axis=1)\n",
    "    df_te = pd.concat([X_te_all, y_te], axis=1)\n",
    "\n",
    "    # Kandidaten\n",
    "    configs = [\n",
    "        {\"name\":\"all_features\",          \"drop_calc\":False, \"extra_drop\":[],                          \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+opt+extras\", \"drop_calc\":True,  \"extra_drop\":[\"ps_ind_14\",\"ps_car_10_cat\"], \"add_extras\":True},\n",
    "        {\"name\":\"drop_calc_only\",       \"drop_calc\":True,  \"extra_drop\":[],                          \"add_extras\":False},\n",
    "        {\"name\":\"drop_calc+extras\",     \"drop_calc\":True,  \"extra_drop\":[],                          \"add_extras\":True},\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for cfg in configs:\n",
    "        X_tr = make_feature_set(df_tr, drop_calc=cfg[\"drop_calc\"], extra_drop=cfg[\"extra_drop\"], add_extras=cfg[\"add_extras\"])\n",
    "        if TE_CAT:\n",
    "            auc_cv, pr_cv = cv_scores_te(X_tr, y_tr.loc[X_tr.index], C=C, CV=CV)\n",
    "        else:\n",
    "            auc_cv, pr_cv = cv_scores_ohe(X_tr, y_tr.loc[X_tr.index], C=C, CV=CV)\n",
    "        rows.append({\"name\":cfg[\"name\"], \"n_features\":int(X_tr.shape[1]), \"cv_auc\":float(auc_cv), \"cv_pr_auc\":float(pr_cv),\n",
    "                     \"drop_calc\":cfg[\"drop_calc\"], \"extra_drop\":cfg[\"extra_drop\"], \"add_extras\":cfg[\"add_extras\"],\n",
    "                     \"te_cat\": TE_CAT})\n",
    "\n",
    "    res = pd.DataFrame(rows).sort_values([\"cv_auc\",\"cv_pr_auc\"], ascending=False)\n",
    "    res_path = reports/\"feature_gate_scores.csv\"; res.to_csv(res_path, index=False)\n",
    "\n",
    "    # Bestes Set → Holdout\n",
    "    best = res.iloc[0].to_dict()\n",
    "    X_tr_best = make_feature_set(df_tr, drop_calc=best[\"drop_calc\"], extra_drop=best[\"extra_drop\"], add_extras=best[\"add_extras\"])\n",
    "    X_te_best = make_feature_set(df_te, drop_calc=best[\"drop_calc\"], extra_drop=best[\"extra_drop\"], add_extras=best[\"add_extras\"])\n",
    "\n",
    "    if TE_CAT:\n",
    "        Xtr_cat, Xva_cat, Xtr_bin, Xva_bin, Xtr_num, Xva_num = _prep_te_blocks(X_tr_best, X_te_best)\n",
    "        tr_te, va_te = _kfold_target_encode(Xtr_cat, y_tr.loc[X_tr_best.index], Xva_cat, n_splits=CV, alpha=10)\n",
    "        Xtr_fin = pd.concat([Xtr_num, Xtr_bin, tr_te], axis=1)\n",
    "        Xva_fin = pd.concat([Xva_num, Xva_bin, va_te], axis=1)\n",
    "        clf = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=RND)\n",
    "        clf.fit(Xtr_fin, y_tr.loc[X_tr_best.index])\n",
    "        proba_hold = clf.predict_proba(Xva_fin)[:,1]\n",
    "    else:\n",
    "        cat, bin_, num = split_cols(X_tr_best.columns)\n",
    "        pipe = Pipeline([(\"pre\", build_pre(cat, bin_, num)), (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=RND))])\n",
    "        m = pipe.fit(X_tr_best, y_tr.loc[X_tr_best.index])\n",
    "        proba_hold = m.predict_proba(X_te_best)[:,1]\n",
    "\n",
    "    hold_auc = roc_auc_score(y_te.loc[X_te_best.index], proba_hold)\n",
    "    hold_pr  = average_precision_score(y_te.loc[X_te_best.index], proba_hold)\n",
    "\n",
    "    # Gegenprobe: all_features\n",
    "    X_tr_all = make_feature_set(df_tr, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    X_te_all = make_feature_set(df_te, drop_calc=False, extra_drop=[], add_extras=False)\n",
    "    if TE_CAT:\n",
    "        Xtr_catA, Xva_catA, Xtr_binA, Xva_binA, Xtr_numA, Xva_numA = _prep_te_blocks(X_tr_all, X_te_all)\n",
    "        tr_teA, va_teA = _kfold_target_encode(Xtr_catA, y_tr.loc[X_tr_all.index], Xva_catA, n_splits=CV, alpha=10)\n",
    "        Xtr_all_fin = pd.concat([Xtr_numA, Xtr_binA, tr_teA], axis=1)\n",
    "        Xva_all_fin = pd.concat([Xva_numA, Xva_binA, va_teA], axis=1)\n",
    "        clfA = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=RND)\n",
    "        clfA.fit(Xtr_all_fin, y_tr.loc[X_tr_all.index])\n",
    "        proba_hold_all = clfA.predict_proba(Xva_all_fin)[:,1]\n",
    "    else:\n",
    "        catA, binA, numA = split_cols(X_tr_all.columns)\n",
    "        pipe_all = Pipeline([(\"pre\", build_pre(catA, binA, numA)), (\"clf\", LogisticRegression(penalty=\"l2\", solver=\"saga\", C=C, class_weight=\"balanced\", max_iter=4000, random_state=RND))])\n",
    "        m_all = pipe_all.fit(X_tr_all, y_tr.loc[X_tr_all.index])\n",
    "        proba_hold_all = m_all.predict_proba(X_te_all)[:,1]\n",
    "\n",
    "    hold_auc_all = roc_auc_score(y_te.loc[X_te_all.index], proba_hold_all)\n",
    "    hold_pr_all  = average_precision_score(y_te.loc[X_te_all.index], proba_hold_all)\n",
    "\n",
    "    # Fallback-Regel\n",
    "    if (hold_auc + 1e-12) < (hold_auc_all - 0.002) or (hold_pr + 1e-12) < (hold_pr_all - 0.002):\n",
    "        chosen = {\"name\":\"all_features\", \"drop_calc\":False, \"extra_drop\":[], \"add_extras\":False, \"cv_auc\":None, \"cv_pr_auc\":None,\n",
    "                  \"holdout_auc\":float(hold_auc_all), \"holdout_pr_auc\":float(hold_pr_all), \"te_cat\": TE_CAT}\n",
    "        X_choose = X_tr_all.columns\n",
    "    else:\n",
    "        chosen = {\"name\":best[\"name\"], \"drop_calc\":best[\"drop_calc\"], \"extra_drop\":best[\"extra_drop\"], \"add_extras\":best[\"add_extras\"],\n",
    "                  \"cv_auc\":float(best[\"cv_auc\"]), \"cv_pr_auc\":float(best[\"cv_pr_auc\"]), \"holdout_auc\":float(hold_auc), \"holdout_pr_auc\":float(hold_pr),\n",
    "                  \"te_cat\": TE_CAT}\n",
    "        X_choose = X_tr_best.columns\n",
    "\n",
    "    # Optionaler GBM-Check (verwendet OHE-Preprocessing)\n",
    "    gbm_out = None\n",
    "    if GBM_CHECK:\n",
    "        catB, binB, numB = split_cols(X_tr_best.columns); preB = build_pre(catB, binB, numB)\n",
    "        XtrB = preB.fit_transform(X_tr_best); XvaB = preB.transform(X_te_best)\n",
    "        gbm_best = holdout_gbm_check(XtrB, y_tr.loc[X_tr_best.index], XvaB, y_te.loc[X_te_best.index])\n",
    "        catC, binC, numC = split_cols(X_tr_all.columns); preC = build_pre(catC, binC, numC)\n",
    "        XtrC = preC.fit_transform(X_tr_all); XvaC = preC.transform(X_te_all)\n",
    "        gbm_all  = holdout_gbm_check(XtrC, y_tr.loc[X_tr_all.index], XvaC, y_te.loc[X_te_all.index])\n",
    "        gbm_out = {\"best\": gbm_best, \"all\": gbm_all}\n",
    "\n",
    "    # Outputs\n",
    "    feat_path = reports/\"features_selected.csv\"\n",
    "    pd.Series(pd.Index(X_choose), name=\"raw_feature\").to_csv(feat_path, index=False)\n",
    "\n",
    "    meta = {\"random_state\":RND, \"cv_splits\":CV, \"C\":C, \"n_rows\":int(len(df)), \"sample_n\":int(len(df)),\n",
    "            \"te_cat\": TE_CAT, \"gbm_check\": bool(GBM_CHECK),\n",
    "            \"scores_path\":str(res_path), \"features_path\":str(feat_path), \"chosen\":chosen,\n",
    "            \"gbm_holdout\": gbm_out}\n",
    "    (reports/\"feature_gate_meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "\n",
    "   # Console-Report\n",
    "    print(\"\\nFEATURE-GATE fertig.\")\n",
    "    print(f\"Train n={len(df_tr):,}, Holdout n={len(df_te):,}, CV={CV}, C={C}, TE_CAT={int(TE_CAT)}, GBM_CHECK={int(GBM_CHECK)}\")\n",
    "    print(\"Scores (CV):\\n\" + res.head(6).to_string(index=False))\n",
    "    print(f\"\\nHoldout {best['name']}:   AUC={hold_auc:.4f}  PR-AUC={hold_pr:.4f}\")\n",
    "    print(f\"Holdout all_features: AUC={hold_auc_all:.4f}  PR-AUC={hold_pr_all:.4f}\")\n",
    "    if gbm_out:\n",
    "        print(\"\\n[GBM] Holdout AUC/PR (Best):\", gbm_out[\"best\"])\n",
    "        print(\"[GBM] Holdout AUC/PR (All): \", gbm_out[\"all\"])\n",
    "    print(f\"\\nGewählt: {chosen['name']}\")\n",
    "    print(f\"→ Spalten: {feat_path}\")\n",
    "    print(f\"→ Scores:  {res_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
