{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] SPEED=MEDIUM CV=5 N_EST=4000 EARLY_STOP=100 MODELS=['lgbm', 'xgb'] IMB=spw\n",
      "Lade Datensatz lokal...\n",
      "Datensatz erfolgreich geladen\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006742 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1884\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's auc: 0.613508\tvalid_0's binary_logloss: 0.153924\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1880\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's auc: 0.608396\tvalid_0's binary_logloss: 0.153995\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1875\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's auc: 0.610238\tvalid_0's binary_logloss: 0.154136\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007509 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1876\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.622416\tvalid_0's binary_logloss: 0.154019\n",
      "[LightGBM] [Info] Number of positive: 5852, number of negative: 154148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1883\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036575 -> initscore=-3.271130\n",
      "[LightGBM] [Info] Start training from score -3.271130\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's auc: 0.614768\tvalid_0's binary_logloss: 0.153858\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, warnings, time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, brier_score_loss, precision_recall_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# ---------- Paths ----------\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    ROOT = Path.cwd() if Path.cwd().name not in (\"notebooks\",\"tools\",\"tests\") else Path.cwd().parent\n",
    "\n",
    "REPORTS_IN  = Path(os.getenv(\"REPORTS_IN\")  or (ROOT / \"reports\"))         # shared inputs (split, features)\n",
    "REPORTS_OUT = Path(os.getenv(\"REPORTS_OUT\") or (ROOT / \"reports_Lucas\"))   # your outputs\n",
    "REPORTS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "# ---------- Speed/Profile ----------\n",
    "SPEED = os.getenv(\"SPEED\", \"MEDIUM\").upper().strip()\n",
    "def speed_cfg():\n",
    "    cfg = dict(CV=5, N_EST=6000, EARLY_STOP=200, MODELS=[\"lgbm\",\"xgb\"], LR=0.03)\n",
    "    if SPEED == \"FAST\":\n",
    "        cfg.update(CV=3, N_EST=2000, EARLY_STOP=50, MODELS=[\"lgbm\"], LR=0.05)\n",
    "    elif SPEED == \"MEDIUM\":\n",
    "        cfg.update(CV=5, N_EST=4000, EARLY_STOP=100)\n",
    "    elif SPEED == \"FULL\":\n",
    "        cfg.update(CV=5, N_EST=8000, EARLY_STOP=300)\n",
    "    return cfg\n",
    "\n",
    "CFG        = speed_cfg()\n",
    "RND        = int(os.getenv(\"RND\", \"42\"))\n",
    "CV         = int(os.getenv(\"CV\", str(CFG[\"CV\"])))\n",
    "N_EST      = int(os.getenv(\"N_EST\", str(CFG[\"N_EST\"])))\n",
    "ESR        = int(os.getenv(\"EARLY_STOP\", str(CFG[\"EARLY_STOP\"])))\n",
    "MODELS     = [m.strip() for m in os.getenv(\"MODELS\", \",\".join(CFG[\"MODELS\"])).split(\",\") if m.strip()]\n",
    "IMB        = os.getenv(\"IMB\", \"spw\").lower()   # 'iso' (LGBM is_unbalance) or 'spw' (scale_pos_weight)\n",
    "LR         = float(os.getenv(\"LR\", str(CFG[\"LR\"])))\n",
    "MEMBER     = os.getenv(\"MEMBER\", \"Lucas\")\n",
    "\n",
    "# ---------- Utils ----------\n",
    "def split_cols(cols):\n",
    "    cat = [c for c in cols if str(c).endswith(\"_cat\")]\n",
    "    bin_ = [c for c in cols if str(c).endswith(\"_bin\")]\n",
    "    num  = [c for c in cols if c not in cat and c not in bin_ and c != \"target\"]\n",
    "    return cat, bin_, num\n",
    "\n",
    "def load_selected_feature_list():\n",
    "    f = REPORTS_IN / \"features_selected.csv\"\n",
    "    if not f.exists():\n",
    "        raise FileNotFoundError(f\"Missing {f}. Run feature-gate first.\")\n",
    "    s = pd.read_csv(f)\n",
    "    if \"raw_feature\" not in s.columns:\n",
    "        raise ValueError(\"features_selected.csv must have column 'raw_feature'.\")\n",
    "    return s[\"raw_feature\"].astype(str).tolist()\n",
    "\n",
    "def fe_extras(X, selected):\n",
    "    X = X.copy()\n",
    "    if \"missing_count\" in selected:\n",
    "        X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    if \"sum_all_bin\" in selected:\n",
    "        b = [c for c in X.columns if str(c).endswith(\"_bin\")]\n",
    "        X[\"sum_all_bin\"] = X[b].sum(axis=1) if b else 0\n",
    "    return X\n",
    "\n",
    "def prep_for_trees(X: pd.DataFrame, selected_cols):\n",
    "    X = fe_extras(X, selected_cols)\n",
    "    keep = [c for c in selected_cols if c in X.columns]\n",
    "    missing = [c for c in selected_cols if c not in X.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] ignoring {len(missing)} missing selected feature(s).\")\n",
    "    X = X[keep].copy()\n",
    "    cat, _, _ = split_cols(X.columns)\n",
    "    for c in cat:\n",
    "        try: X[c] = X[c].astype(\"category\")\n",
    "        except: pass\n",
    "    return X, cat\n",
    "\n",
    "def scale_pos_weight(y):\n",
    "    pos = int((y==1).sum()); neg = int((y==0).sum())\n",
    "    return float(neg / max(pos,1))\n",
    "\n",
    "# ---------- XGB helpers ----------\n",
    "def xgb_train_predict(Xtr, ytr, Xva, yva, Xte=None, params=None, seed=RND, lr=LR):\n",
    "    import xgboost as xgb\n",
    "    params = dict(params or {})\n",
    "    p = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": params.pop(\"tree_method\", \"hist\"),\n",
    "        \"eta\": params.pop(\"learning_rate\", lr),\n",
    "        \"max_depth\": int(params.pop(\"max_depth\", 6)),\n",
    "        \"min_child_weight\": float(params.pop(\"min_child_weight\", 2.0)),\n",
    "        \"subsample\": float(params.pop(\"subsample\", 0.9)),\n",
    "        \"colsample_bytree\": float(params.pop(\"colsample_bytree\", 0.9)),\n",
    "        \"lambda\": float(params.pop(\"reg_lambda\", 1.0)),\n",
    "        \"alpha\": float(params.pop(\"reg_alpha\", 0.0)),\n",
    "        \"gamma\": float(params.pop(\"gamma\", 0.0)),\n",
    "        \"seed\": int(params.pop(\"seed\", seed)),\n",
    "        \"nthread\": -1,\n",
    "    }\n",
    "    if IMB == \"spw\":\n",
    "        p[\"scale_pos_weight\"] = float(params.pop(\"scale_pos_weight\", 1.0))\n",
    "\n",
    "    meta = {\"use_ohe\": False, \"ohe_cols\": None}\n",
    "    cats = [c for c in Xtr.columns if str(c).endswith(\"_cat\")]\n",
    "    try:\n",
    "        dtr = xgb.DMatrix(Xtr, label=ytr, enable_categorical=True)\n",
    "        dva = xgb.DMatrix(Xva, label=yva, enable_categorical=True)\n",
    "        dte = xgb.DMatrix(Xte, enable_categorical=True) if Xte is not None else None\n",
    "        p[\"enable_categorical\"] = True\n",
    "    except Exception:\n",
    "        meta[\"use_ohe\"] = True\n",
    "        dXtr = pd.get_dummies(Xtr, columns=cats, dummy_na=True)\n",
    "        dXva = pd.get_dummies(Xva, columns=cats, dummy_na=True).reindex(columns=dXtr.columns, fill_value=0)\n",
    "        dtr = xgb.DMatrix(dXtr, label=ytr)\n",
    "        dva = xgb.DMatrix(dXva, label=yva)\n",
    "        dte = None\n",
    "        if Xte is not None:\n",
    "            dXte = pd.get_dummies(Xte, columns=cats, dummy_na=True).reindex(columns=dXtr.columns, fill_value=0)\n",
    "            dte = xgb.DMatrix(dXte)\n",
    "        meta[\"ohe_cols\"] = list(dXtr.columns)\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=p, dtrain=dtr, num_boost_round=N_EST,\n",
    "        evals=[(dva, \"valid\")], early_stopping_rounds=ESR, verbose_eval=False\n",
    "    )\n",
    "    pred_va = booster.predict(dva, iteration_range=(0, (booster.best_iteration or N_EST)))\n",
    "    pred_te = booster.predict(dte, iteration_range=(0, (booster.best_iteration or N_EST))) if Xte is not None else None\n",
    "\n",
    "    imp = booster.get_score(importance_type=\"gain\")\n",
    "    if meta[\"use_ohe\"]:\n",
    "        fi = pd.Series(imp, name=\"gain\").sort_values(ascending=False)\n",
    "        fi.index.name = \"feature\"\n",
    "    else:\n",
    "        names = list(Xtr.columns)\n",
    "        pairs = []\n",
    "        for k, v in imp.items():\n",
    "            if k.startswith(\"f\") and k[1:].isdigit():\n",
    "                idx = int(k[1:]); name = names[idx] if 0 <= idx < len(names) else k\n",
    "            else:\n",
    "                name = k\n",
    "            pairs.append((name, v))\n",
    "        fi = pd.Series(dict(pairs), name=\"gain\").sort_values(ascending=False)\n",
    "\n",
    "    return booster, pred_va, pred_te, fi, meta\n",
    "\n",
    "def xgb_predict_time_ms_per_1k(booster, X, meta):\n",
    "    import xgboost as xgb\n",
    "    t0 = time.perf_counter()\n",
    "    if meta[\"use_ohe\"]:\n",
    "        cats = [c for c in X.columns if str(c).endswith(\"_cat\")]\n",
    "        dX = pd.get_dummies(X, columns=cats, dummy_na=True).reindex(columns=meta[\"ohe_cols\"], fill_value=0)\n",
    "        d = xgb.DMatrix(dX)\n",
    "    else:\n",
    "        d = xgb.DMatrix(X, enable_categorical=True)\n",
    "    _ = booster.predict(d, iteration_range=(0, (booster.best_iteration or N_EST)))\n",
    "    dt = time.perf_counter() - t0\n",
    "    return 1000 * dt / (len(X)/1000)\n",
    "\n",
    "# ---------- CV (OOF) ----------\n",
    "def oof_cv(model_name, base_params, X, y, cat_cols):\n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RND)\n",
    "    oof = np.zeros(len(y), dtype=float)\n",
    "\n",
    "    for tr, va in skf.split(X, y):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]; ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "\n",
    "        if model_name == \"lgbm\":\n",
    "            from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "            imb_kwargs = {\"is_unbalance\": True} if IMB == \"iso\" else {\"is_unbalance\": False}\n",
    "\n",
    "            defaults = {\n",
    "                \"n_estimators\": N_EST, \"random_state\": RND, \"n_jobs\": -1,\n",
    "                \"learning_rate\": LR, \"num_leaves\": 128, \"max_depth\": -1,\n",
    "                \"min_child_samples\": 20, \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "                \"reg_lambda\": 1.0, \"reg_alpha\": 0.0, \"max_bin\": 511,\n",
    "                \"feature_pre_filter\": False, **imb_kwargs\n",
    "            }\n",
    "            if base_params:\n",
    "                defaults.update(base_params)   # safe merge\n",
    "\n",
    "            clf = LGBMClassifier(**defaults)\n",
    "            clf.fit(\n",
    "                Xtr, ytr, eval_set=[(Xva, yva)], eval_metric=\"auc\",\n",
    "                categorical_feature=[c for c in cat_cols if c in Xtr.columns],\n",
    "                callbacks=[early_stopping(ESR), log_evaluation(0)]\n",
    "            )\n",
    "            oof[va] = clf.predict_proba(Xva)[:,1]\n",
    "\n",
    "        elif model_name == \"xgb\":\n",
    "            spw = scale_pos_weight(ytr) if IMB == \"spw\" else 1.0\n",
    "            _, pred_va, _, _, _ = xgb_train_predict(\n",
    "                Xtr, ytr, Xva, yva, params={**(base_params or {}), \"scale_pos_weight\": spw}\n",
    "            )\n",
    "            oof[va] = pred_va\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model\")\n",
    "\n",
    "    pr = average_precision_score(y, oof)\n",
    "    roc = roc_auc_score(y, oof)\n",
    "    br  = brier_score_loss(y, oof)\n",
    "    return dict(pr_auc=float(pr), roc_auc=float(roc), brier=float(br), oof=oof)\n",
    "\n",
    "# ---------- Final fit ----------\n",
    "def fit_final(model_name, params, Xtr, ytr, Xte, yte, cat_cols):\n",
    "    # small val split for ES\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(Xtr, ytr, test_size=0.1, stratify=ytr, random_state=RND)\n",
    "\n",
    "    if model_name == \"lgbm\":\n",
    "        from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "        imb_kwargs = {\"is_unbalance\": True} if IMB == \"iso\" else {\"is_unbalance\": False}\n",
    "\n",
    "        defaults = {\n",
    "            \"n_estimators\": N_EST, \"random_state\": RND, \"n_jobs\": -1,\n",
    "            \"learning_rate\": LR, \"num_leaves\": 128, \"max_depth\": -1,\n",
    "            \"min_child_samples\": 20, \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0, \"reg_alpha\": 0.0, \"max_bin\": 511,\n",
    "            \"feature_pre_filter\": False, **imb_kwargs\n",
    "        }\n",
    "        if params:\n",
    "            defaults.update(params)  # safe merge\n",
    "\n",
    "        clf = LGBMClassifier(**defaults)\n",
    "        t0 = time.perf_counter()\n",
    "        clf.fit(\n",
    "            X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=\"auc\",\n",
    "            categorical_feature=[c for c in cat_cols if c in X_tr.columns],\n",
    "            callbacks=[early_stopping(ESR), log_evaluation(0)]\n",
    "        )\n",
    "        fit_time_s = time.perf_counter() - t0\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        proba = clf.predict_proba(Xte)[:,1]\n",
    "        pred_ms_per_1k = 1000 * (time.perf_counter() - t1) / (len(Xte)/1000)\n",
    "\n",
    "        try:\n",
    "            gain = clf.booster_.feature_importance(importance_type=\"gain\")\n",
    "            names = clf.booster_.feature_name()\n",
    "            fi = pd.Series(gain, index=names, name=\"gain\").sort_values(ascending=False)\n",
    "        except Exception:\n",
    "            fi = pd.Series(clf.feature_importances_, index=Xtr.columns, name=\"split\").sort_values(ascending=False)\n",
    "\n",
    "        meta = {\n",
    "            \"encoder\": \"native(LGBM)\",\n",
    "            \"best_iteration\": getattr(clf, \"best_iteration_\", None),\n",
    "            \"n_trees\": getattr(clf, \"n_estimators_\", None),\n",
    "            \"fit_time_s\": float(fit_time_s),\n",
    "            \"predict_time_ms_per_1k\": float(pred_ms_per_1k),\n",
    "            \"model_obj\": clf\n",
    "        }\n",
    "\n",
    "    else:  # xgb\n",
    "        spw = scale_pos_weight(y_tr) if IMB == \"spw\" else 1.0\n",
    "        t0 = time.perf_counter()\n",
    "        booster, _, proba, fi, xmeta = xgb_train_predict(\n",
    "            X_tr, y_tr, X_val, y_val, Xte, params={**(params or {}), \"scale_pos_weight\": spw}\n",
    "        )\n",
    "        fit_time_s = time.perf_counter() - t0\n",
    "        pred_ms_per_1k = xgb_predict_time_ms_per_1k(booster, Xte, xmeta)\n",
    "        meta = {\n",
    "            \"encoder\": \"native(XGB)\" if not xmeta[\"use_ohe\"] else \"OHE(XGB-fallback)\",\n",
    "            \"best_iteration\": getattr(booster, \"best_iteration\", None),\n",
    "            \"n_trees\": getattr(booster, \"best_ntree_limit\", None),\n",
    "            \"fit_time_s\": float(fit_time_s),\n",
    "            \"predict_time_ms_per_1k\": float(pred_ms_per_1k),\n",
    "            \"model_obj\": booster\n",
    "        }\n",
    "\n",
    "    hold = dict(\n",
    "        pr_auc=float(average_precision_score(yte, proba)),\n",
    "        roc_auc=float(roc_auc_score(yte, proba)),\n",
    "        brier=float(brier_score_loss(yte, proba))\n",
    "    )\n",
    "    return proba, hold, fi, meta\n",
    "\n",
    "# ---------- Plots ----------\n",
    "def save_pr_curve(y_true, proba, out_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    prec, rec, _ = precision_recall_curve(y_true, proba)\n",
    "    ap = average_precision_score(y_true, proba)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(rec, prec, label=f'AP={ap:.4f}')\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall')\n",
    "    plt.xlim([0,1]); plt.ylim([0,1]); plt.grid(True, alpha=0.3); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def save_calibration(y_true, proba, out_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    prob_true, prob_pred = calibration_curve(y_true, proba, n_bins=20, strategy=\"quantile\")\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot([0,1],[0,1],'--',label='Perfect')\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Observed'); plt.title('Calibration')\n",
    "    plt.grid(True, alpha=0.3); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def save_top20_importance(fi: pd.Series, out_path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    if fi is None or fi.empty: return\n",
    "    top = fi.head(20).iloc[::-1]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.barh(top.index, top.values)\n",
    "    plt.xlabel('Gain'); plt.title('Top-20 Feature Importance')\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "# ---------- Param sampler ----------\n",
    "def sample_params(model_name, n):\n",
    "    rng = np.random.default_rng(RND); out = []\n",
    "    if model_name == \"lgbm\":\n",
    "        for _ in range(n):\n",
    "            out.append(dict(\n",
    "                learning_rate=LR,\n",
    "                num_leaves=int(rng.integers(64, 256)),\n",
    "                max_depth=int(rng.integers(-1, 12)),\n",
    "                min_child_samples=int(rng.integers(10, 80)),\n",
    "                subsample=float(rng.uniform(0.7, 1.0)),\n",
    "                colsample_bytree=float(rng.uniform(0.7, 1.0)),\n",
    "                reg_lambda=float(rng.uniform(0.0, 2.0)),\n",
    "                reg_alpha=float(rng.uniform(0.0, 1.0)),\n",
    "                is_unbalance=(IMB==\"iso\"),\n",
    "                max_bin=511, feature_pre_filter=False\n",
    "            ))\n",
    "    elif model_name == \"xgb\":\n",
    "        for _ in range(n):\n",
    "            out.append(dict(\n",
    "                learning_rate=LR, tree_method=\"hist\",\n",
    "                max_depth=int(rng.integers(3, 10)),\n",
    "                min_child_weight=float(rng.uniform(1.0, 8.0)),\n",
    "                subsample=float(rng.uniform(0.7, 1.0)),\n",
    "                colsample_bytree=float(rng.uniform(0.7, 1.0)),\n",
    "                reg_lambda=float(rng.uniform(0.0, 2.0)),\n",
    "                reg_alpha=float(rng.uniform(0.0, 1.0)),\n",
    "                gamma=float(rng.uniform(0.0, 2.0)),\n",
    "                scale_pos_weight=1.0  # set per-fold if IMB==\"spw\"\n",
    "            ))\n",
    "    return out\n",
    "\n",
    "# ---------- Main ----------\n",
    "def main():\n",
    "    print(f\"[SETUP] SPEED={SPEED} CV={CV} N_EST={N_EST} EARLY_STOP={ESR} MODELS={MODELS} IMB={IMB}\")\n",
    "\n",
    "    split_p = REPORTS_IN / \"split_indices.json\"\n",
    "    feats_p = REPORTS_IN / \"features_selected.csv\"\n",
    "    assert split_p.exists() and feats_p.exists(), \"Missing split and/or features files in reports.\"\n",
    "\n",
    "    split = json.loads(split_p.read_text())\n",
    "    selected = load_selected_feature_list()\n",
    "\n",
    "    df = load_and_save_data().replace(-1, np.nan)\n",
    "    X_tr_all = df.loc[split[\"train\"]].drop(columns=[\"target\"])\n",
    "    y_tr     = df.loc[split[\"train\"], \"target\"].astype(int)\n",
    "    X_te_all = df.loc[split[\"test\"]].drop(columns=[\"target\"])\n",
    "    y_te     = df.loc[split[\"test\"], \"target\"].astype(int)\n",
    "\n",
    "    Xtr, cat_cols = prep_for_trees(X_tr_all, selected)\n",
    "    Xte, _        = prep_for_trees(X_te_all, selected)\n",
    "\n",
    "    available = []\n",
    "    for m in MODELS:\n",
    "        if m == \"lgbm\":\n",
    "            try: import lightgbm  # noqa\n",
    "            except Exception: print(\"[WARN] LightGBM not available. Skipping.\")\n",
    "            else: available.append(\"lgbm\")\n",
    "        elif m == \"xgb\":\n",
    "            try: import xgboost  # noqa\n",
    "            except Exception: print(\"[WARN] XGBoost not available. Skipping.\")\n",
    "            else: available.append(\"xgb\")\n",
    "    if not available:\n",
    "        raise RuntimeError(\"No models available.\")\n",
    "\n",
    "    # OOF baselines\n",
    "    baselines = {}\n",
    "    for m in available:\n",
    "        res = oof_cv(m, {}, Xtr, y_tr, cat_cols)\n",
    "        baselines[m] = {k: float(v) for k, v in res.items() if k in (\"pr_auc\",\"roc_auc\",\"brier\")}\n",
    "        pd.DataFrame({\"oof\": res[\"oof\"]}).to_csv(REPORTS_OUT/f\"oof_{m}.csv\", index=False)\n",
    "    pd.DataFrame.from_dict(baselines, orient=\"index\").reset_index().rename(\n",
    "        columns={\"index\":\"model\"}).to_csv(REPORTS_OUT/\"baselines_summary.csv\", index=False)\n",
    "\n",
    "    # pick best by PR-AUC\n",
    "    best_model = max(baselines.items(), key=lambda kv: kv[1][\"pr_auc\"])[0]\n",
    "\n",
    "    # tiny random search around best\n",
    "    n_iters = 4 if SPEED == \"FAST\" else (8 if SPEED == \"MEDIUM\" else 20)\n",
    "    tuning_rows, best_cv = [], {\"score\": -1.0, \"model\": None, \"params\": None}\n",
    "    for params in sample_params(best_model, n_iters):\n",
    "        res = oof_cv(best_model, params, Xtr, y_tr, cat_cols)\n",
    "        tuning_rows.append({\n",
    "            \"model\": best_model, \"pr_auc\": res[\"pr_auc\"], \"roc_auc\": res[\"roc_auc\"],\n",
    "            \"brier\": res[\"brier\"], \"params\": json.dumps(params)\n",
    "        })\n",
    "        if res[\"pr_auc\"] > best_cv[\"score\"]:\n",
    "            best_cv = {\"score\": float(res[\"pr_auc\"]), \"model\": best_model, \"params\": params}\n",
    "\n",
    "    if tuning_rows:\n",
    "        pd.DataFrame(tuning_rows).to_csv(REPORTS_OUT/\"tuning_log.csv\", index=False)\n",
    "\n",
    "    best_params = best_cv[\"params\"] if best_cv[\"model\"] else {}\n",
    "\n",
    "    # final fit + holdout\n",
    "    proba, hold, fi, meta = fit_final(best_model, best_params, Xtr, y_tr, Xte, y_te, cat_cols)\n",
    "\n",
    "    # save predictions, metrics, FI, plots\n",
    "    pd.DataFrame({\"proba\": proba, \"y_true\": y_te.values}).to_csv(REPORTS_OUT/\"holdout_preds.csv\", index=False)\n",
    "    pd.DataFrame([{\n",
    "        \"model\": best_model, \"params\": json.dumps(best_params),\n",
    "        \"pr_auc\": hold[\"pr_auc\"], \"roc_auc\": hold[\"roc_auc\"], \"brier\": hold[\"brier\"]\n",
    "    }]).to_csv(REPORTS_OUT/\"holdout_metrics.csv\", index=False)\n",
    "\n",
    "    if fi is not None and not fi.empty:\n",
    "        fi.reset_index().rename(columns={\"index\":\"feature\"}).to_csv(REPORTS_OUT/\"fi_gain.csv\", index=False)\n",
    "\n",
    "    save_pr_curve(y_te.values, proba, REPORTS_OUT/\"plot_pr.png\")\n",
    "    save_calibration(y_te.values, proba, REPORTS_OUT/\"plot_calibration.png\")\n",
    "    save_top20_importance(fi, REPORTS_OUT/\"plot_fi_top20.png\")\n",
    "\n",
    "    # team summary row\n",
    "    row = {\n",
    "        \"member\": MEMBER,\n",
    "        \"model_name\": best_model.upper(),\n",
    "        \"encoder\": meta[\"encoder\"],\n",
    "        \"split_path\": str(split_p),\n",
    "        \"feature_recipe\": \"selected_from_feature_gate\",\n",
    "        \"seed\": RND, \"cv_folds\": CV,\n",
    "        \"hold_auc\": hold[\"roc_auc\"], \"hold_ap\": hold[\"pr_auc\"], \"hold_brier\": hold[\"brier\"],\n",
    "        \"cv_auc_mean\": None, \"cv_ap_mean\": None,\n",
    "        \"early_stopping\": True,\n",
    "        \"best_iteration\": meta[\"best_iteration\"],\n",
    "        \"n_trees\": meta[\"n_trees\"],\n",
    "        \"fit_time_s\": meta[\"fit_time_s\"],\n",
    "        \"predict_time_ms_per_1k\": meta[\"predict_time_ms_per_1k\"],\n",
    "        \"params_json\": json.dumps(best_params),\n",
    "    }\n",
    "    out_csv = REPORTS_OUT/\"team_model_summary.csv\"\n",
    "    pd.DataFrame([row]).to_csv(out_csv, mode=\"a\", index=False, header=not out_csv.exists())\n",
    "\n",
    "    print(\"\\n[BASELINES]\", json.dumps(baselines, indent=2))\n",
    "    if best_cv[\"model\"]:\n",
    "        print(f\"[CV] tuned={best_model}  PR-AUC={best_cv['score']:.5f}\")\n",
    "    print(f\"[HOLDOUT] PR-AUC={hold['pr_auc']:.5f}  ROC-AUC={hold['roc_auc']:.5f}  Brier={hold['brier']:.5f}\")\n",
    "    print(f\"Saved to: {REPORTS_OUT}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as ex:\n",
    "        print(\"ERROR:\", type(ex).__name__, \"-\", ex)\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
