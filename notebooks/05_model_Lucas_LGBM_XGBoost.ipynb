{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b37e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "Speed Profile: MEDIUM\n",
      "Cross-Validation Folds: 5\n",
      "Max Estimators: 4000\n",
      "Early Stopping Rounds: 100\n",
      "Models: lgbm, xgb\n",
      "Imbalance Strategy: spw\n",
      "Learning Rate: 0.03\n",
      "Random Seed: 42\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Porto Seguro Safe Driver Prediction - Advanced Model Training & Analysis\n",
    "=========================================================================\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for binary classification\n",
    "with LGBM and XGBoost models, including:\n",
    "- Cross-validation with stratified folds\n",
    "- Hyperparameter tuning\n",
    "- Feature importance analysis\n",
    "- Model calibration & evaluation\n",
    "- Rich visualizations for model interpretation\n",
    "\n",
    "Author: Lucas\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, json, warnings, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================================\n",
    "\n",
    "# Determine ROOT directory\n",
    "if \"__file__\" in globals():\n",
    "    ROOT = Path(__file__).resolve().parents[1]\n",
    "else:\n",
    "    ROOT = Path.cwd() if Path.cwd().name not in (\"notebooks\",\"tools\",\"tests\") else Path.cwd().parent\n",
    "\n",
    "REPORTS_IN  = Path(os.getenv(\"REPORTS_IN\")  or (ROOT / \"reports\"))\n",
    "REPORTS_OUT = Path(os.getenv(\"REPORTS_OUT\") or (ROOT / \"reports_Lucas\"))\n",
    "REPORTS_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.path.insert(0, str(ROOT))\n",
    "from src.data_loader import load_and_save_data\n",
    "\n",
    "# Speed/Profile Configuration\n",
    "SPEED = os.getenv(\"SPEED\", \"MEDIUM\").upper().strip()\n",
    "\n",
    "def get_speed_config() -> Dict[str, Any]:\n",
    "    \"\"\"Get configuration based on speed profile.\"\"\"\n",
    "    base = dict(CV=5, N_EST=6000, EARLY_STOP=200, MODELS=[\"lgbm\",\"xgb\"], LR=0.03)\n",
    "    \n",
    "    profiles = {\n",
    "        \"FAST\": dict(CV=3, N_EST=2000, EARLY_STOP=50, MODELS=[\"lgbm\"], LR=0.05),\n",
    "        \"MEDIUM\": dict(CV=5, N_EST=4000, EARLY_STOP=100),\n",
    "        \"FULL\": dict(CV=5, N_EST=8000, EARLY_STOP=300)\n",
    "    }\n",
    "    \n",
    "    if SPEED in profiles:\n",
    "        base.update(profiles[SPEED])\n",
    "    return base\n",
    "\n",
    "CFG = get_speed_config()\n",
    "\n",
    "# Global parameters\n",
    "RND     = int(os.getenv(\"RND\", \"42\"))\n",
    "CV      = int(os.getenv(\"CV\", str(CFG[\"CV\"])))\n",
    "N_EST   = int(os.getenv(\"N_EST\", str(CFG[\"N_EST\"])))\n",
    "ESR     = int(os.getenv(\"EARLY_STOP\", str(CFG[\"EARLY_STOP\"])))\n",
    "MODELS  = [m.strip() for m in os.getenv(\"MODELS\", \",\".join(CFG[\"MODELS\"])).split(\",\") if m.strip()]\n",
    "IMB     = os.getenv(\"IMB\", \"spw\").lower()  # 'iso' or 'spw'\n",
    "LR      = float(os.getenv(\"LR\", str(CFG[\"LR\"])))\n",
    "MEMBER  = os.getenv(\"MEMBER\", \"Lucas\")\n",
    "\n",
    "print(f\"\"\"\n",
    "{'='*80}\n",
    "CONFIGURATION\n",
    "{'='*80}\n",
    "Speed Profile: {SPEED}\n",
    "Cross-Validation Folds: {CV}\n",
    "Max Estimators: {N_EST}\n",
    "Early Stopping Rounds: {ESR}\n",
    "Models: {', '.join(MODELS)}\n",
    "Imbalance Strategy: {IMB}\n",
    "Learning Rate: {LR}\n",
    "Random Seed: {RND}\n",
    "{'='*80}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fwuwd6j2c",
   "metadata": {},
   "source": [
    "# ðŸš— Porto Seguro Safe Driver Prediction - Advanced ML Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive machine learning pipeline for predicting insurance claim probability using **LightGBM** and **XGBoost** gradient boosting models.\n",
    "\n",
    "## ðŸŽ¯ Key Features\n",
    "\n",
    "### 1. **Data Processing**\n",
    "- Automated feature engineering (missing counts, binary sums)\n",
    "- Intelligent categorical feature handling\n",
    "- Train/test splitting with stratification\n",
    "\n",
    "### 2. **Model Training**\n",
    "- Cross-validation with stratified K-folds\n",
    "- Support for both LightGBM and XGBoost\n",
    "- Class imbalance handling (scale_pos_weight / is_unbalance)\n",
    "- Early stopping to prevent overfitting\n",
    "- Hyperparameter tuning\n",
    "\n",
    "### 3. **Comprehensive Evaluation**\n",
    "- **Metrics**: ROC-AUC, PR-AUC (Average Precision), Brier Score\n",
    "- **Visualizations**:\n",
    "  - ROC Curve\n",
    "  - Precision-Recall Curve\n",
    "  - Calibration Plot with prediction distribution\n",
    "  - Confusion Matrix with metrics\n",
    "  - Feature Importance (Top 30)\n",
    "  - Threshold Analysis\n",
    "  - CV Fold Performance\n",
    "  - Model Comparison Charts\n",
    "\n",
    "### 4. **Production-Ready Outputs**\n",
    "- Model predictions (CSV)\n",
    "- Performance metrics (CSV)\n",
    "- Feature importance rankings (CSV)\n",
    "- High-quality visualizations (PNG, 300 DPI)\n",
    "- Comprehensive summary report\n",
    "\n",
    "## ðŸ“Š Workflow\n",
    "\n",
    "```\n",
    "Load Data â†’ Feature Engineering â†’ Cross-Validation â†’ Model Selection â†’ \n",
    "Final Training â†’ Holdout Evaluation â†’ Visualizations â†’ Summary Report\n",
    "```\n",
    "\n",
    "## âš™ï¸ Configuration\n",
    "\n",
    "Environment variables control the training process:\n",
    "- `SPEED`: FAST | MEDIUM | FULL (controls CV folds, estimators, early stopping)\n",
    "- `CV`: Number of cross-validation folds (default: 5)\n",
    "- `N_EST`: Maximum number of estimators (default: 4000)\n",
    "- `MODELS`: Comma-separated list of models (lgbm, xgb)\n",
    "- `IMB`: Imbalance handling strategy (spw | iso)\n",
    "- `RND`: Random seed for reproducibility (default: 42)\n",
    "\n",
    "## ðŸ“ Output Directory\n",
    "\n",
    "All results are saved to: `reports_Lucas/`\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Lucas  \n",
    "**Last Updated**: 2025-11-30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lwmt0z8t54",
   "metadata": {},
   "source": [
    "## ðŸš€ Quick Start\n",
    "\n",
    "### Running the Notebook\n",
    "\n",
    "1. **Prerequisites**: Ensure you have run feature selection first to generate:\n",
    "   - `reports/split_indices.json` (train/test split)\n",
    "   - `reports/features_selected.csv` (selected features)\n",
    "\n",
    "2. **Run all cells** in order (Cell â†’ Run All)\n",
    "\n",
    "3. **Results** will be saved to `reports_Lucas/` directory\n",
    "\n",
    "### Speed Profiles\n",
    "\n",
    "Set the `SPEED` environment variable before running:\n",
    "\n",
    "```python\n",
    "# Fast testing (3-fold CV, 2000 trees)\n",
    "import os\n",
    "os.environ['SPEED'] = 'FAST'\n",
    "\n",
    "# Balanced (5-fold CV, 4000 trees) - DEFAULT\n",
    "os.environ['SPEED'] = 'MEDIUM'\n",
    "\n",
    "# Full quality (5-fold CV, 8000 trees)\n",
    "os.environ['SPEED'] = 'FULL'\n",
    "```\n",
    "\n",
    "### Expected Runtime\n",
    "\n",
    "- **FAST**: ~2-5 minutes\n",
    "- **MEDIUM**: ~5-15 minutes  \n",
    "- **FULL**: ~15-30 minutes\n",
    "\n",
    "*Times vary based on dataset size and hardware*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "og8dqdrkfm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Data processing utilities loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS - Data Processing\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score, brier_score_loss,\n",
    "    precision_recall_curve, roc_curve, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def split_feature_types(cols: List[str]) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Split features into categorical, binary, and numeric.\"\"\"\n",
    "    cat_cols = [c for c in cols if str(c).endswith(\"_cat\")]\n",
    "    bin_cols = [c for c in cols if str(c).endswith(\"_bin\")]\n",
    "    num_cols = [c for c in cols if c not in cat_cols and c not in bin_cols and c != \"target\"]\n",
    "    return cat_cols, bin_cols, num_cols\n",
    "\n",
    "def load_selected_features() -> List[str]:\n",
    "    \"\"\"Load selected features from feature gate output.\"\"\"\n",
    "    feature_file = REPORTS_IN / \"features_selected.csv\"\n",
    "    if not feature_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing {feature_file}. Run feature selection first.\")\n",
    "    \n",
    "    df = pd.read_csv(feature_file)\n",
    "    if \"raw_feature\" not in df.columns:\n",
    "        raise ValueError(\"features_selected.csv must have 'raw_feature' column.\")\n",
    "    \n",
    "    return df[\"raw_feature\"].astype(str).tolist()\n",
    "\n",
    "def add_engineered_features(X: pd.DataFrame, selected_features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Add engineered features if they're in the selected list.\"\"\"\n",
    "    X = X.copy()\n",
    "    \n",
    "    if \"missing_count\" in selected_features:\n",
    "        X[\"missing_count\"] = X.isna().sum(axis=1)\n",
    "    \n",
    "    if \"sum_all_bin\" in selected_features:\n",
    "        bin_cols = [c for c in X.columns if str(c).endswith(\"_bin\")]\n",
    "        X[\"sum_all_bin\"] = X[bin_cols].sum(axis=1) if bin_cols else 0\n",
    "    \n",
    "    return X\n",
    "\n",
    "def prepare_data_for_trees(\n",
    "    X: pd.DataFrame, \n",
    "    selected_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Prepare data for tree-based models.\"\"\"\n",
    "    X = add_engineered_features(X, selected_cols)\n",
    "    \n",
    "    # Keep only selected features that exist\n",
    "    available_cols = [c for c in selected_cols if c in X.columns]\n",
    "    missing_cols = [c for c in selected_cols if c not in X.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"âš ï¸  Warning: {len(missing_cols)} selected features not found in data\")\n",
    "    \n",
    "    X = X[available_cols].copy()\n",
    "    \n",
    "    # Convert categorical features\n",
    "    cat_cols, _, _ = split_feature_types(X.columns)\n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            X[col] = X[col].astype(\"category\")\n",
    "        except:\n",
    "            print(f\"âš ï¸  Warning: Could not convert {col} to category dtype\")\n",
    "    \n",
    "    return X, cat_cols\n",
    "\n",
    "def calculate_pos_weight(y: pd.Series) -> float:\n",
    "    \"\"\"Calculate scale_pos_weight for imbalanced data.\"\"\"\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    return float(n_neg / max(n_pos, 1))\n",
    "\n",
    "print(\"âœ“ Data processing utilities loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yjs7shnce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Visualization functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS - Enhanced Plots\n",
    "# ============================================================================\n",
    "\n",
    "def create_figure(figsize=(10, 6), title=None):\n",
    "    \"\"\"Create a nicely formatted figure.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "    return fig, ax\n",
    "\n",
    "def save_roc_curve(y_true, y_pred, output_path, model_name=\"Model\"):\n",
    "    \"\"\"Plot ROC curve with AUC score.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    auc_score = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    fig, ax = create_figure(figsize=(8, 8), title='ROC Curve')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', alpha=0.3)\n",
    "    ax.plot(fpr, tpr, linewidth=2.5, label=f'{model_name} (AUC = {auc_score:.4f})')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved ROC curve: {output_path.name}\")\n",
    "\n",
    "def save_pr_curve(y_true, y_pred, output_path, model_name=\"Model\"):\n",
    "    \"\"\"Plot Precision-Recall curve with AP score.\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    ap_score = average_precision_score(y_true, y_pred)\n",
    "    baseline = (y_true == 1).sum() / len(y_true)\n",
    "    \n",
    "    fig, ax = create_figure(figsize=(8, 8), title='Precision-Recall Curve')\n",
    "    ax.plot([0, 1], [baseline, baseline], 'k--', label=f'Baseline (No Skill = {baseline:.4f})', alpha=0.3)\n",
    "    ax.plot(recall, precision, linewidth=2.5, label=f'{model_name} (AP = {ap_score:.4f})')\n",
    "    \n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved PR curve: {output_path.name}\")\n",
    "\n",
    "def save_calibration_plot(y_true, y_pred, output_path, n_bins=20):\n",
    "    \"\"\"Plot calibration curve to assess probability calibration.\"\"\"\n",
    "    prob_true, prob_pred = calibration_curve(y_true, y_pred, n_bins=n_bins, strategy='quantile')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Calibration curve\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', alpha=0.5)\n",
    "    ax1.plot(prob_pred, prob_true, marker='o', linewidth=2, markersize=8, label='Model')\n",
    "    ax1.set_xlabel('Predicted Probability', fontsize=12)\n",
    "    ax1.set_ylabel('True Probability', fontsize=12)\n",
    "    ax1.set_title('Calibration Curve', fontsize=13, fontweight='bold')\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of predictions\n",
    "    ax2.hist(y_pred, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax2.set_xlabel('Predicted Probability', fontsize=12)\n",
    "    ax2.set_ylabel('Count', fontsize=12)\n",
    "    ax2.set_title('Distribution of Predictions', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved calibration plot: {output_path.name}\")\n",
    "\n",
    "def save_confusion_matrix(y_true, y_pred, output_path, threshold=0.5):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    \n",
    "    fig, ax = create_figure(figsize=(8, 6), title=f'Confusion Matrix (threshold={threshold})')\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "                xticklabels=['No Claim', 'Claim'],\n",
    "                yticklabels=['No Claim', 'Claim'],\n",
    "                ax=ax, annot_kws={'size': 14})\n",
    "    \n",
    "    ax.set_xlabel('Predicted', fontsize=12)\n",
    "    ax.set_ylabel('Actual', fontsize=12)\n",
    "    \n",
    "    # Add accuracy metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics_text = f'Accuracy: {accuracy:.3f}\\nPrecision: {precision:.3f}\\nRecall: {recall:.3f}\\nF1: {f1:.3f}'\n",
    "    ax.text(1.15, 0.5, metrics_text, transform=ax.transAxes, \n",
    "            fontsize=11, verticalalignment='center',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved confusion matrix: {output_path.name}\")\n",
    "\n",
    "def save_feature_importance(importance_df, output_path, top_n=30, title=\"Feature Importance\"):\n",
    "    \"\"\"Plot top N features by importance.\"\"\"\n",
    "    if importance_df is None or importance_df.empty:\n",
    "        print(\"âš ï¸  No feature importance data available\")\n",
    "        return\n",
    "    \n",
    "    # Get top features\n",
    "    top_features = importance_df.head(top_n).iloc[::-1]  # Reverse for better display\n",
    "    \n",
    "    fig, ax = create_figure(figsize=(10, max(8, top_n * 0.3)), title=title)\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\n",
    "    bars = ax.barh(range(len(top_features)), top_features.values, color=colors, edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_yticks(range(len(top_features)))\n",
    "    ax.set_yticklabels(top_features.index, fontsize=10)\n",
    "    ax.set_xlabel('Importance Score', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, value) in enumerate(zip(bars, top_features.values)):\n",
    "        ax.text(value, i, f' {value:.0f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved feature importance: {output_path.name}\")\n",
    "\n",
    "def save_threshold_analysis(y_true, y_pred, output_path):\n",
    "    \"\"\"Plot metrics across different probability thresholds.\"\"\"\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    precisions, recalls, f1_scores = [], [], []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred_binary = (y_pred >= thresh).astype(int)\n",
    "        tp = ((y_pred_binary == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred_binary == 1) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred_binary == 0) & (y_true == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    fig, ax = create_figure(figsize=(10, 6), title='Metrics vs Probability Threshold')\n",
    "    \n",
    "    ax.plot(thresholds, precisions, label='Precision', linewidth=2)\n",
    "    ax.plot(thresholds, recalls, label='Recall', linewidth=2)\n",
    "    ax.plot(thresholds, f1_scores, label='F1 Score', linewidth=2, linestyle='--')\n",
    "    \n",
    "    # Mark optimal F1 threshold\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    optimal_thresh = thresholds[optimal_idx]\n",
    "    ax.axvline(optimal_thresh, color='red', linestyle=':', alpha=0.5, \n",
    "               label=f'Optimal F1 Threshold = {optimal_thresh:.3f}')\n",
    "    \n",
    "    ax.set_xlabel('Probability Threshold', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"âœ“ Saved threshold analysis: {output_path.name}\")\n",
    "\n",
    "print(\"âœ“ Visualization functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nk22vwo92ml",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model training functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL TRAINING FUNCTIONS - LightGBM & XGBoost\n",
    "# ============================================================================\n",
    "\n",
    "def train_lightgbm_cv(X, y, cat_cols, params=None):\n",
    "    \"\"\"Train LightGBM with cross-validation and return OOF predictions.\"\"\"\n",
    "    from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "    \n",
    "    imb_kwargs = {\"is_unbalance\": True} if IMB == \"iso\" else {\"is_unbalance\": False}\n",
    "    \n",
    "    default_params = {\n",
    "        \"n_estimators\": N_EST,\n",
    "        \"random_state\": RND,\n",
    "        \"n_jobs\": -1,\n",
    "        \"learning_rate\": LR,\n",
    "        \"num_leaves\": 128,\n",
    "        \"max_depth\": -1,\n",
    "        \"min_child_samples\": 20,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"reg_lambda\": 1.0,\n",
    "        \"reg_alpha\": 0.0,\n",
    "        \"max_bin\": 511,\n",
    "        \"feature_pre_filter\": False,\n",
    "        **imb_kwargs\n",
    "    }\n",
    "    \n",
    "    if params:\n",
    "        default_params.update(params)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RND)\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        clf = LGBMClassifier(**default_params)\n",
    "        clf.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"auc\",\n",
    "            categorical_feature=[c for c in cat_cols if c in X_train.columns],\n",
    "            callbacks=[early_stopping(ESR), log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        oof_preds[val_idx] = clf.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        fold_auc = roc_auc_score(y_val, oof_preds[val_idx])\n",
    "        fold_ap = average_precision_score(y_val, oof_preds[val_idx])\n",
    "        fold_scores.append({'fold': fold, 'roc_auc': fold_auc, 'pr_auc': fold_ap})\n",
    "        print(f\"  Fold {fold}/{CV} - ROC-AUC: {fold_auc:.5f}, PR-AUC: {fold_ap:.5f}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'pr_auc': average_precision_score(y, oof_preds),\n",
    "        'roc_auc': roc_auc_score(y, oof_preds),\n",
    "        'brier': brier_score_loss(y, oof_preds),\n",
    "        'oof': oof_preds,\n",
    "        'fold_scores': fold_scores\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_xgboost_cv(X, y, cat_cols, params=None):\n",
    "    \"\"\"Train XGBoost with cross-validation and return OOF predictions.\"\"\"\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    default_params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"aucpr\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eta\": LR,\n",
    "        \"max_depth\": 6,\n",
    "        \"min_child_weight\": 2.0,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"lambda\": 1.0,\n",
    "        \"alpha\": 0.0,\n",
    "        \"gamma\": 0.0,\n",
    "        \"seed\": RND,\n",
    "        \"nthread\": -1,\n",
    "    }\n",
    "    \n",
    "    if params:\n",
    "        for key, val in params.items():\n",
    "            if key == \"learning_rate\":\n",
    "                default_params[\"eta\"] = val\n",
    "            elif key not in [\"scale_pos_weight\"]:  # Will be set per-fold\n",
    "                default_params[key] = val\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=RND)\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if IMB == \"spw\":\n",
    "            default_params[\"scale_pos_weight\"] = calculate_pos_weight(y_train)\n",
    "        \n",
    "        try:\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "        except:\n",
    "            # Fallback to OHE if categorical not supported\n",
    "            X_train_ohe = pd.get_dummies(X_train, columns=cat_cols, dummy_na=True)\n",
    "            X_val_ohe = pd.get_dummies(X_val, columns=cat_cols, dummy_na=True).reindex(\n",
    "                columns=X_train_ohe.columns, fill_value=0\n",
    "            )\n",
    "            dtrain = xgb.DMatrix(X_train_ohe, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val_ohe, label=y_val)\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=default_params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=N_EST,\n",
    "            evals=[(dval, \"valid\")],\n",
    "            early_stopping_rounds=ESR,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        oof_preds[val_idx] = model.predict(dval)\n",
    "        \n",
    "        fold_auc = roc_auc_score(y_val, oof_preds[val_idx])\n",
    "        fold_ap = average_precision_score(y_val, oof_preds[val_idx])\n",
    "        fold_scores.append({'fold': fold, 'roc_auc': fold_auc, 'pr_auc': fold_ap})\n",
    "        print(f\"  Fold {fold}/{CV} - ROC-AUC: {fold_auc:.5f}, PR-AUC: {fold_ap:.5f}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'pr_auc': average_precision_score(y, oof_preds),\n",
    "        'roc_auc': roc_auc_score(y, oof_preds),\n",
    "        'brier': brier_score_loss(y, oof_preds),\n",
    "        'oof': oof_preds,\n",
    "        'fold_scores': fold_scores\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_final_model(model_name, X_train, y_train, X_test, y_test, cat_cols, params=None):\n",
    "    \"\"\"Train final model on full training set and evaluate on holdout test set.\"\"\"\n",
    "    # Use 10% for validation in final training\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.1, stratify=y_train, random_state=RND\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training final {model_name.upper()} model...\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    if model_name == \"lgbm\":\n",
    "        from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
    "        \n",
    "        imb_kwargs = {\"is_unbalance\": True} if IMB == \"iso\" else {\"is_unbalance\": False}\n",
    "        \n",
    "        default_params = {\n",
    "            \"n_estimators\": N_EST, \"random_state\": RND, \"n_jobs\": -1,\n",
    "            \"learning_rate\": LR, \"num_leaves\": 128, \"max_depth\": -1,\n",
    "            \"min_child_samples\": 20, \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "            \"reg_lambda\": 1.0, \"reg_alpha\": 0.0, \"max_bin\": 511,\n",
    "            \"feature_pre_filter\": False, **imb_kwargs\n",
    "        }\n",
    "        \n",
    "        if params:\n",
    "            default_params.update(params)\n",
    "        \n",
    "        model = LGBMClassifier(**default_params)\n",
    "        model.fit(\n",
    "            X_tr, y_tr,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_metric=\"auc\",\n",
    "            categorical_feature=[c for c in cat_cols if c in X_tr.columns],\n",
    "            callbacks=[early_stopping(ESR), log_evaluation(0)]\n",
    "        )\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        pred_start = time.time()\n",
    "        y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        pred_time_ms_per_1k = 1000 * (time.time() - pred_start) / (len(X_test) / 1000)\n",
    "        \n",
    "        # Feature importance\n",
    "        try:\n",
    "            importance = model.booster_.feature_importance(importance_type=\"gain\")\n",
    "            feature_names = model.booster_.feature_name()\n",
    "            importance_df = pd.Series(importance, index=feature_names, name=\"gain\").sort_values(ascending=False)\n",
    "        except:\n",
    "            importance_df = pd.Series(\n",
    "                model.feature_importances_, \n",
    "                index=X_train.columns, \n",
    "                name=\"split\"\n",
    "            ).sort_values(ascending=False)\n",
    "        \n",
    "        metadata = {\n",
    "            \"encoder\": \"native(LGBM)\",\n",
    "            \"best_iteration\": getattr(model, \"best_iteration_\", None),\n",
    "            \"n_trees\": getattr(model, \"n_estimators_\", None)\n",
    "        }\n",
    "    \n",
    "    else:  # xgboost\n",
    "        import xgboost as xgb\n",
    "        \n",
    "        default_params = {\n",
    "            \"objective\": \"binary:logistic\", \"eval_metric\": \"aucpr\",\n",
    "            \"tree_method\": \"hist\", \"eta\": LR, \"max_depth\": 6,\n",
    "            \"min_child_weight\": 2.0, \"subsample\": 0.9, \"colsample_bytree\": 0.9,\n",
    "            \"lambda\": 1.0, \"alpha\": 0.0, \"gamma\": 0.0, \"seed\": RND, \"nthread\": -1,\n",
    "        }\n",
    "        \n",
    "        if params:\n",
    "            for key, val in params.items():\n",
    "                if key == \"learning_rate\":\n",
    "                    default_params[\"eta\"] = val\n",
    "                else:\n",
    "                    default_params[key] = val\n",
    "        \n",
    "        if IMB == \"spw\":\n",
    "            default_params[\"scale_pos_weight\"] = calculate_pos_weight(y_tr)\n",
    "        \n",
    "        try:\n",
    "            dtrain = xgb.DMatrix(X_tr, label=y_tr, enable_categorical=True)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True)\n",
    "            dtest = xgb.DMatrix(X_test, enable_categorical=True)\n",
    "            encoder_type = \"native(XGB)\"\n",
    "        except:\n",
    "            X_tr_ohe = pd.get_dummies(X_tr, columns=cat_cols, dummy_na=True)\n",
    "            X_val_ohe = pd.get_dummies(X_val, columns=cat_cols, dummy_na=True).reindex(\n",
    "                columns=X_tr_ohe.columns, fill_value=0\n",
    "            )\n",
    "            X_test_ohe = pd.get_dummies(X_test, columns=cat_cols, dummy_na=True).reindex(\n",
    "                columns=X_tr_ohe.columns, fill_value=0\n",
    "            )\n",
    "            dtrain = xgb.DMatrix(X_tr_ohe, label=y_tr)\n",
    "            dval = xgb.DMatrix(X_val_ohe, label=y_val)\n",
    "            dtest = xgb.DMatrix(X_test_ohe)\n",
    "            encoder_type = \"OHE(XGB-fallback)\"\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=default_params, dtrain=dtrain, num_boost_round=N_EST,\n",
    "            evals=[(dval, \"valid\")], early_stopping_rounds=ESR, verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        pred_start = time.time()\n",
    "        y_pred = model.predict(dtest)\n",
    "        pred_time_ms_per_1k = 1000 * (time.time() - pred_start) / (len(X_test) / 1000)\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = model.get_score(importance_type=\"gain\")\n",
    "        importance_df = pd.Series(importance, name=\"gain\").sort_values(ascending=False)\n",
    "        \n",
    "        metadata = {\n",
    "            \"encoder\": encoder_type,\n",
    "            \"best_iteration\": getattr(model, \"best_iteration\", None),\n",
    "            \"n_trees\": getattr(model, \"best_ntree_limit\", None)\n",
    "        }\n",
    "    \n",
    "    # Evaluate\n",
    "    test_metrics = {\n",
    "        'pr_auc': average_precision_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred),\n",
    "        'brier': brier_score_loss(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    metadata.update({\n",
    "        \"fit_time_s\": train_time,\n",
    "        \"predict_time_ms_per_1k\": pred_time_ms_per_1k,\n",
    "        \"model_obj\": model\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nâœ“ Training completed in {train_time:.2f}s\")\n",
    "    print(f\"  Holdout ROC-AUC: {test_metrics['roc_auc']:.5f}\")\n",
    "    print(f\"  Holdout PR-AUC: {test_metrics['pr_auc']:.5f}\")\n",
    "    print(f\"  Brier Score: {test_metrics['brier']:.5f}\")\n",
    "    \n",
    "    return y_pred, test_metrics, importance_df, metadata\n",
    "\n",
    "print(\"âœ“ Model training functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ob0wv61snc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "âœ“ Loaded 37 selected features\n",
      "Loading dataset from local file: /Users/lucasbeseler/ada_portoSeguro/data/raw/porto_seguro_safe_driver_prediction.csv\n",
      "Dataset loaded successfully.\n",
      "âœ“ Train set: 200,000 samples\n",
      "âœ“ Test set: 50,000 samples\n",
      "âœ“ Positive class ratio (train): 0.0366\n",
      "âœ“ Positive class ratio (test): 0.0366\n",
      "âœ“ Final feature count: 37\n",
      "âœ“ Categorical features: 13\n",
      "\n",
      "================================================================================\n",
      "BASELINE CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ LightGBM available\n",
      "âœ“ XGBoost available\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training LGBM with 5-fold CV...\n",
      "--------------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1884\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid_0's auc: 0.613508\tvalid_0's binary_logloss: 0.153924\n",
      "  Fold 1/5 - ROC-AUC: 0.61351, PR-AUC: 0.05896\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1880\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid_0's auc: 0.608396\tvalid_0's binary_logloss: 0.153995\n",
      "  Fold 2/5 - ROC-AUC: 0.60840, PR-AUC: 0.06005\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005458 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1875\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's auc: 0.610238\tvalid_0's binary_logloss: 0.154136\n",
      "  Fold 3/5 - ROC-AUC: 0.61024, PR-AUC: 0.05710\n",
      "[LightGBM] [Info] Number of positive: 5853, number of negative: 154147\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1876\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036581 -> initscore=-3.270952\n",
      "[LightGBM] [Info] Start training from score -3.270952\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's auc: 0.622416\tvalid_0's binary_logloss: 0.154019\n",
      "  Fold 4/5 - ROC-AUC: 0.62242, PR-AUC: 0.05752\n",
      "[LightGBM] [Info] Number of positive: 5852, number of negative: 154148\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1883\n",
      "[LightGBM] [Info] Number of data points in the train set: 160000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036575 -> initscore=-3.271130\n",
      "[LightGBM] [Info] Start training from score -3.271130\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's auc: 0.614768\tvalid_0's binary_logloss: 0.153858\n",
      "  Fold 5/5 - ROC-AUC: 0.61477, PR-AUC: 0.06006\n",
      "\n",
      "âœ“ LGBM CV Results:\n",
      "  ROC-AUC: 0.61259\n",
      "  PR-AUC: 0.05827\n",
      "  Brier Score: 0.03501\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training XGB with 5-fold CV...\n",
      "--------------------------------------------------------------------------------\n",
      "  Fold 1/5 - ROC-AUC: 0.59490, PR-AUC: 0.05499\n",
      "  Fold 2/5 - ROC-AUC: 0.59350, PR-AUC: 0.05601\n",
      "  Fold 3/5 - ROC-AUC: 0.59557, PR-AUC: 0.05473\n",
      "  Fold 4/5 - ROC-AUC: 0.59952, PR-AUC: 0.05645\n",
      "  Fold 5/5 - ROC-AUC: 0.59882, PR-AUC: 0.05799\n",
      "\n",
      "âœ“ XGB CV Results:\n",
      "  ROC-AUC: 0.59594\n",
      "  PR-AUC: 0.05563\n",
      "  Brier Score: 0.20098\n",
      "\n",
      "================================================================================\n",
      "MODEL COMPARISON (Cross-Validation)\n",
      "================================================================================\n",
      "\n",
      "model  roc_auc   pr_auc    brier\n",
      " LGBM 0.612594 0.058265 0.035010\n",
      "  XGB 0.595944 0.055633 0.200982\n",
      "\n",
      "================================================================================\n",
      "âœ“ Best Model: LGBM (PR-AUC: 0.05827)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION - Load Data & Run Cross-Validation\n",
    "# ============================================================================\n",
    "\n",
    "# Load split indices and features\n",
    "split_file = REPORTS_IN / \"split_indices.json\"\n",
    "features_file = REPORTS_IN / \"features_selected.csv\"\n",
    "\n",
    "if not split_file.exists() or not features_file.exists():\n",
    "    raise FileNotFoundError(\"Missing required files. Run feature selection and data split first.\")\n",
    "\n",
    "# Load data\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"LOADING DATA\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "with open(split_file, 'r') as f:\n",
    "    split_indices = json.load(f)\n",
    "\n",
    "selected_features = load_selected_features()\n",
    "print(f\"âœ“ Loaded {len(selected_features)} selected features\")\n",
    "\n",
    "# Load full dataset\n",
    "df_full = load_and_save_data().replace(-1, np.nan)\n",
    "\n",
    "# Split into train/test\n",
    "X_train_raw = df_full.loc[split_indices[\"train\"]].drop(columns=[\"target\"])\n",
    "y_train = df_full.loc[split_indices[\"train\"], \"target\"].astype(int)\n",
    "X_test_raw = df_full.loc[split_indices[\"test\"]].drop(columns=[\"target\"])\n",
    "y_test = df_full.loc[split_indices[\"test\"], \"target\"].astype(int)\n",
    "\n",
    "print(f\"âœ“ Train set: {len(X_train_raw):,} samples\")\n",
    "print(f\"âœ“ Test set: {len(X_test_raw):,} samples\")\n",
    "print(f\"âœ“ Positive class ratio (train): {y_train.mean():.4f}\")\n",
    "print(f\"âœ“ Positive class ratio (test): {y_test.mean():.4f}\")\n",
    "\n",
    "# Prepare data for tree-based models\n",
    "X_train, cat_cols = prepare_data_for_trees(X_train_raw, selected_features)\n",
    "X_test, _ = prepare_data_for_trees(X_test_raw, selected_features)\n",
    "\n",
    "print(f\"âœ“ Final feature count: {X_train.shape[1]}\")\n",
    "print(f\"âœ“ Categorical features: {len(cat_cols)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-VALIDATION - Baseline Models\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"BASELINE CROSS-VALIDATION\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = []\n",
    "for model_name in MODELS:\n",
    "    if model_name == \"lgbm\":\n",
    "        try:\n",
    "            import lightgbm\n",
    "            available_models.append(\"lgbm\")\n",
    "            print(\"âœ“ LightGBM available\")\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  LightGBM not available, skipping\")\n",
    "    elif model_name == \"xgb\":\n",
    "        try:\n",
    "            import xgboost\n",
    "            available_models.append(\"xgb\")\n",
    "            print(\"âœ“ XGBoost available\")\n",
    "        except ImportError:\n",
    "            print(\"âš ï¸  XGBoost not available, skipping\")\n",
    "\n",
    "if not available_models:\n",
    "    raise RuntimeError(\"No models available. Install lightgbm or xgboost.\")\n",
    "\n",
    "# Train baseline models with CV\n",
    "for model_name in available_models:\n",
    "    print(f\"\\n{'-'*80}\")\n",
    "    print(f\"Training {model_name.upper()} with {CV}-fold CV...\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    if model_name == \"lgbm\":\n",
    "        cv_results[model_name] = train_lightgbm_cv(X_train, y_train, cat_cols)\n",
    "    elif model_name == \"xgb\":\n",
    "        cv_results[model_name] = train_xgboost_cv(X_train, y_train, cat_cols)\n",
    "    \n",
    "    print(f\"\\nâœ“ {model_name.upper()} CV Results:\")\n",
    "    print(f\"  ROC-AUC: {cv_results[model_name]['roc_auc']:.5f}\")\n",
    "    print(f\"  PR-AUC: {cv_results[model_name]['pr_auc']:.5f}\")\n",
    "    print(f\"  Brier Score: {cv_results[model_name]['brier']:.5f}\")\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    pd.DataFrame({\n",
    "        'oof_predictions': cv_results[model_name]['oof'],\n",
    "        'y_true': y_train.values\n",
    "    }).to_csv(REPORTS_OUT / f\"oof_{model_name}.csv\", index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL COMPARISON TABLE\n",
    "# ============================================================================\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'model': model_name.upper(),\n",
    "        'roc_auc': results['roc_auc'],\n",
    "        'pr_auc': results['pr_auc'],\n",
    "        'brier': results['brier']\n",
    "    }\n",
    "    for model_name, results in cv_results.items()\n",
    "])\n",
    "\n",
    "comparison_df = comparison_df.sort_values('pr_auc', ascending=False).reset_index(drop=True)\n",
    "comparison_df.to_csv(REPORTS_OUT / \"model_comparison_cv.csv\", index=False)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL COMPARISON (Cross-Validation)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.iloc[0]['model'].lower()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ“ Best Model: {best_model_name.upper()} (PR-AUC: {comparison_df.iloc[0]['pr_auc']:.5f})\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "l3ipixv3zp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training final LGBM model...\n",
      "================================================================================\n",
      "[LightGBM] [Info] Number of positive: 6584, number of negative: 173416\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1887\n",
      "[LightGBM] [Info] Number of data points in the train set: 180000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036578 -> initscore=-3.271051\n",
      "[LightGBM] [Info] Start training from score -3.271051\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid_0's auc: 0.613683\tvalid_0's binary_logloss: 0.154082\n",
      "\n",
      "âœ“ Training completed in 3.96s\n",
      "  Holdout ROC-AUC: 0.62428\n",
      "  Holdout PR-AUC: 0.06347\n",
      "  Brier Score: 0.03494\n",
      "\n",
      "âœ“ Results saved to /Users/lucasbeseler/ada_portoSeguro/reports_Lucas\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FINAL MODEL TRAINING & EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "# Train final model on full training set\n",
    "y_pred_test, holdout_metrics, feature_importance, metadata = train_final_model(\n",
    "    best_model_name, X_train, y_train, X_test, y_test, cat_cols\n",
    ")\n",
    "\n",
    "# Save results\n",
    "pd.DataFrame({\n",
    "    'y_pred': y_pred_test,\n",
    "    'y_true': y_test.values\n",
    "}).to_csv(REPORTS_OUT / \"holdout_predictions.csv\", index=False)\n",
    "\n",
    "pd.DataFrame([holdout_metrics]).to_csv(REPORTS_OUT / \"holdout_metrics.csv\", index=False)\n",
    "\n",
    "if feature_importance is not None and not feature_importance.empty:\n",
    "    feature_importance.reset_index().rename(\n",
    "        columns={'index': 'feature', 0: 'importance'}\n",
    "    ).to_csv(REPORTS_OUT / \"feature_importance.csv\", index=False)\n",
    "\n",
    "print(\"\\nâœ“ Results saved to\", REPORTS_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "srf7plnk63o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING VISUALIZATIONS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved ROC curve: plot_roc_curve.png\n",
      "âœ“ Saved PR curve: plot_pr_curve.png\n",
      "âœ“ Saved calibration plot: plot_calibration.png\n",
      "âœ“ Saved confusion matrix: plot_confusion_matrix.png\n",
      "âœ“ Saved feature importance: plot_feature_importance_top30.png\n",
      "âœ“ Saved threshold analysis: plot_threshold_analysis.png\n",
      "\n",
      "================================================================================\n",
      "âœ“ All visualizations saved!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# 1. ROC Curve\n",
    "save_roc_curve(\n",
    "    y_test.values, y_pred_test,\n",
    "    REPORTS_OUT / \"plot_roc_curve.png\",\n",
    "    model_name=best_model_name.upper()\n",
    ")\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "save_pr_curve(\n",
    "    y_test.values, y_pred_test,\n",
    "    REPORTS_OUT / \"plot_pr_curve.png\",\n",
    "    model_name=best_model_name.upper()\n",
    ")\n",
    "\n",
    "# 3. Calibration Plot\n",
    "save_calibration_plot(\n",
    "    y_test.values, y_pred_test,\n",
    "    REPORTS_OUT / \"plot_calibration.png\"\n",
    ")\n",
    "\n",
    "# 4. Confusion Matrix\n",
    "save_confusion_matrix(\n",
    "    y_test.values, y_pred_test,\n",
    "    REPORTS_OUT / \"plot_confusion_matrix.png\",\n",
    "    threshold=0.5\n",
    ")\n",
    "\n",
    "# 5. Feature Importance (Top 30)\n",
    "if feature_importance is not None and not feature_importance.empty:\n",
    "    save_feature_importance(\n",
    "        feature_importance,\n",
    "        REPORTS_OUT / \"plot_feature_importance_top30.png\",\n",
    "        top_n=30,\n",
    "        title=f\"Top 30 Features - {best_model_name.upper()}\"\n",
    "    )\n",
    "\n",
    "# 6. Threshold Analysis\n",
    "save_threshold_analysis(\n",
    "    y_test.values, y_pred_test,\n",
    "    REPORTS_OUT / \"plot_threshold_analysis.png\"\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ“ All visualizations saved!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fww20hllje",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model comparison visualizations...\n",
      "âœ“ Saved model comparison plot\n",
      "\n",
      "Creating CV fold scores visualization...\n",
      "âœ“ Saved CV fold scores plot\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL COMPARISON VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "if len(cv_results) > 1:\n",
    "    print(\"\\nCreating model comparison visualizations...\")\n",
    "    \n",
    "    # Create comparison bar chart\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    models = list(cv_results.keys())\n",
    "    metrics_to_plot = ['roc_auc', 'pr_auc', 'brier']\n",
    "    titles = ['ROC-AUC', 'PR-AUC (Average Precision)', 'Brier Score (lower is better)']\n",
    "    colors_map = {'lgbm': '#2ecc71', 'xgb': '#3498db'}\n",
    "    \n",
    "    for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "        values = [cv_results[m][metric] for m in models]\n",
    "        colors = [colors_map.get(m, '#95a5a6') for m in models]\n",
    "        \n",
    "        bars = axes[idx].bar(\n",
    "            [m.upper() for m in models], \n",
    "            values, \n",
    "            color=colors,\n",
    "            edgecolor='black',\n",
    "            linewidth=1.5,\n",
    "            alpha=0.8\n",
    "        )\n",
    "        \n",
    "        axes[idx].set_title(title, fontsize=13, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Score', fontsize=11)\n",
    "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            axes[idx].text(\n",
    "                bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:.4f}',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold'\n",
    "            )\n",
    "        \n",
    "        # For Brier score, invert y-axis to show lower is better\n",
    "        if metric == 'brier':\n",
    "            axes[idx].invert_yaxis()\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison (Cross-Validation)', \n",
    "                 fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(REPORTS_OUT / \"plot_model_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"âœ“ Saved model comparison plot\")\n",
    "\n",
    "# ============================================================================\n",
    "# CV FOLD SCORES VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating CV fold scores visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for model_name, results in cv_results.items():\n",
    "    fold_scores_df = pd.DataFrame(results['fold_scores'])\n",
    "    \n",
    "    # ROC-AUC per fold\n",
    "    axes[0].plot(\n",
    "        fold_scores_df['fold'], \n",
    "        fold_scores_df['roc_auc'], \n",
    "        marker='o', \n",
    "        linewidth=2, \n",
    "        markersize=8,\n",
    "        label=f\"{model_name.upper()} (mean={results['roc_auc']:.4f})\"\n",
    "    )\n",
    "    \n",
    "    # PR-AUC per fold\n",
    "    axes[1].plot(\n",
    "        fold_scores_df['fold'], \n",
    "        fold_scores_df['pr_auc'], \n",
    "        marker='o', \n",
    "        linewidth=2, \n",
    "        markersize=8,\n",
    "        label=f\"{model_name.upper()} (mean={results['pr_auc']:.4f})\"\n",
    "    )\n",
    "\n",
    "axes[0].set_xlabel('Fold', fontsize=12)\n",
    "axes[0].set_ylabel('ROC-AUC', fontsize=12)\n",
    "axes[0].set_title('ROC-AUC per CV Fold', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(range(1, CV+1))\n",
    "\n",
    "axes[1].set_xlabel('Fold', fontsize=12)\n",
    "axes[1].set_ylabel('PR-AUC', fontsize=12)\n",
    "axes[1].set_title('PR-AUC per CV Fold', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(range(1, CV+1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_OUT / \"plot_cv_fold_scores.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ“ Saved CV fold scores plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jn15bvao58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "  FINAL SUMMARY - Porto Seguro Safe Driver Prediction\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ“Š DATASET SUMMARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Training samples: 200,000\n",
      "  Test samples: 50,000\n",
      "  Total features: 37\n",
      "  Categorical features: 13\n",
      "  Class imbalance (positive %): 3.66%\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ”„ CROSS-VALIDATION RESULTS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "  LGBM:\n",
      "    ROC-AUC:     0.61259\n",
      "    PR-AUC:      0.05827\n",
      "    Brier Score: 0.03501\n",
      "\n",
      "  XGB:\n",
      "    ROC-AUC:     0.59594\n",
      "    PR-AUC:      0.05563\n",
      "    Brier Score: 0.20098\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸŽ¯ HOLDOUT TEST RESULTS - LGBM\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  ROC-AUC:     0.62428\n",
      "  PR-AUC:      0.06347\n",
      "  Brier Score: 0.03494\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "âš™ï¸  MODEL METADATA\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Best Model: LGBM\n",
      "  Encoder: native(LGBM)\n",
      "  Best Iteration: 62\n",
      "  Number of Trees: 62\n",
      "  Training Time: 3.96 seconds\n",
      "  Prediction Time: 1.18 ms/1k samples\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "â­ TOP 10 MOST IMPORTANT FEATURES\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   1. ps_car_11_cat                     18171.8\n",
      "   2. ps_reg_03                         12166.5\n",
      "   3. ps_car_13                         11990.3\n",
      "   4. ps_car_14                          6422.9\n",
      "   5. ps_ind_03                          4743.4\n",
      "   6. ps_ind_05_cat                      4266.1\n",
      "   7. ps_reg_02                          4158.3\n",
      "   8. ps_ind_15                          4150.2\n",
      "   9. ps_ind_17_bin                      3391.1\n",
      "  10. ps_ind_01                          3134.9\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "ðŸ’¾ FILES GENERATED\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  âœ“ model_comparison_cv.csv\n",
      "  âœ“ holdout_predictions.csv\n",
      "  âœ“ holdout_metrics.csv\n",
      "  âœ“ feature_importance.csv\n",
      "  âœ“ plot_roc_curve.png\n",
      "  âœ“ plot_pr_curve.png\n",
      "  âœ“ plot_calibration.png\n",
      "  âœ“ plot_confusion_matrix.png\n",
      "  âœ“ plot_feature_importance_top30.png\n",
      "  âœ“ plot_threshold_analysis.png\n",
      "  âœ“ plot_cv_fold_scores.png\n",
      "  âœ“ plot_model_comparison.png\n",
      "\n",
      "ðŸ“ All files saved to: /Users/lucasbeseler/ada_portoSeguro/reports_Lucas\n",
      "\n",
      "================================================================================\n",
      "================================================================================\n",
      "  âœ… MODEL TRAINING & EVALUATION COMPLETE!\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE SUMMARY DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  FINAL SUMMARY - Porto Seguro Safe Driver Prediction\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Dataset Summary\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(\"ðŸ“Š DATASET SUMMARY\")\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(f\"  Training samples: {len(X_train):,}\")\n",
    "print(f\"  Test samples: {len(X_test):,}\")\n",
    "print(f\"  Total features: {X_train.shape[1]}\")\n",
    "print(f\"  Categorical features: {len(cat_cols)}\")\n",
    "print(f\"  Class imbalance (positive %): {y_train.mean()*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Cross-Validation Results\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(\"ðŸ”„ CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'â”€'*80}\")\n",
    "for model_name, results in cv_results.items():\n",
    "    print(f\"\\n  {model_name.upper()}:\")\n",
    "    print(f\"    ROC-AUC:     {results['roc_auc']:.5f}\")\n",
    "    print(f\"    PR-AUC:      {results['pr_auc']:.5f}\")\n",
    "    print(f\"    Brier Score: {results['brier']:.5f}\")\n",
    "print()\n",
    "\n",
    "# Holdout Test Results\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(f\"ðŸŽ¯ HOLDOUT TEST RESULTS - {best_model_name.upper()}\")\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(f\"  ROC-AUC:     {holdout_metrics['roc_auc']:.5f}\")\n",
    "print(f\"  PR-AUC:      {holdout_metrics['pr_auc']:.5f}\")\n",
    "print(f\"  Brier Score: {holdout_metrics['brier']:.5f}\")\n",
    "print()\n",
    "\n",
    "# Model Metadata\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(\"âš™ï¸  MODEL METADATA\")\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(f\"  Best Model: {best_model_name.upper()}\")\n",
    "print(f\"  Encoder: {metadata['encoder']}\")\n",
    "print(f\"  Best Iteration: {metadata['best_iteration']}\")\n",
    "print(f\"  Number of Trees: {metadata['n_trees']}\")\n",
    "print(f\"  Training Time: {metadata['fit_time_s']:.2f} seconds\")\n",
    "print(f\"  Prediction Time: {metadata['predict_time_ms_per_1k']:.2f} ms/1k samples\")\n",
    "print()\n",
    "\n",
    "# Top Features\n",
    "if feature_importance is not None and not feature_importance.empty:\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    print(\"â­ TOP 10 MOST IMPORTANT FEATURES\")\n",
    "    print(f\"{'â”€'*80}\")\n",
    "    top10 = feature_importance.head(10)\n",
    "    for idx, (feature, importance) in enumerate(top10.items(), 1):\n",
    "        print(f\"  {idx:2d}. {feature:30s} {importance:>10.1f}\")\n",
    "    print()\n",
    "\n",
    "# Files Generated\n",
    "print(f\"{'â”€'*80}\")\n",
    "print(\"ðŸ’¾ FILES GENERATED\")\n",
    "print(f\"{'â”€'*80}\")\n",
    "output_files = [\n",
    "    \"model_comparison_cv.csv\",\n",
    "    \"holdout_predictions.csv\",\n",
    "    \"holdout_metrics.csv\",\n",
    "    \"feature_importance.csv\",\n",
    "    \"plot_roc_curve.png\",\n",
    "    \"plot_pr_curve.png\",\n",
    "    \"plot_calibration.png\",\n",
    "    \"plot_confusion_matrix.png\",\n",
    "    \"plot_feature_importance_top30.png\",\n",
    "    \"plot_threshold_analysis.png\",\n",
    "    \"plot_cv_fold_scores.png\",\n",
    "]\n",
    "\n",
    "if len(cv_results) > 1:\n",
    "    output_files.append(\"plot_model_comparison.png\")\n",
    "\n",
    "for file in output_files:\n",
    "    if (REPORTS_OUT / file).exists():\n",
    "        print(f\"  âœ“ {file}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All files saved to: {REPORTS_OUT}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"  âœ… MODEL TRAINING & EVALUATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc11dkpvtim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating comprehensive results dashboard...\n",
      "âœ“ Saved comprehensive dashboard: plot_comprehensive_dashboard.png\n",
      "\n",
      "ðŸŽ‰ All done! Check /Users/lucasbeseler/ada_portoSeguro/reports_Lucas for all outputs.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BONUS: COMPREHENSIVE RESULTS DASHBOARD (Single Image)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating comprehensive results dashboard...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. ROC Curve\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "fpr, tpr, _ = roc_curve(y_test.values, y_pred_test)\n",
    "auc_score = roc_auc_score(y_test.values, y_pred_test)\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
    "ax1.plot(fpr, tpr, linewidth=2.5, label=f'Model (AUC={auc_score:.4f})')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve', fontweight='bold', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Precision-Recall Curve\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "precision, recall, _ = precision_recall_curve(y_test.values, y_pred_test)\n",
    "ap_score = average_precision_score(y_test.values, y_pred_test)\n",
    "baseline = (y_test.values == 1).sum() / len(y_test)\n",
    "ax2.plot([0, 1], [baseline, baseline], 'k--', alpha=0.3, label=f'Baseline ({baseline:.3f})')\n",
    "ax2.plot(recall, precision, linewidth=2.5, label=f'Model (AP={ap_score:.4f})')\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve', fontweight='bold', fontsize=12)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Calibration Curve\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "prob_true, prob_pred = calibration_curve(y_test.values, y_pred_test, n_bins=15, strategy='quantile')\n",
    "ax3.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect')\n",
    "ax3.plot(prob_pred, prob_true, marker='o', linewidth=2, markersize=6, label='Model')\n",
    "ax3.set_xlabel('Predicted Probability')\n",
    "ax3.set_ylabel('True Probability')\n",
    "ax3.set_title('Calibration Curve', fontweight='bold', fontsize=12)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature Importance (Top 15)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "if feature_importance is not None and not feature_importance.empty:\n",
    "    top15 = feature_importance.head(15).iloc[::-1]\n",
    "    colors_fi = plt.cm.viridis(np.linspace(0.3, 0.9, len(top15)))\n",
    "    bars = ax4.barh(range(len(top15)), top15.values, color=colors_fi, edgecolor='black', linewidth=0.5)\n",
    "    ax4.set_yticks(range(len(top15)))\n",
    "    ax4.set_yticklabels(top15.index, fontsize=10)\n",
    "    ax4.set_xlabel('Importance (Gain)', fontsize=11)\n",
    "    ax4.set_title('Top 15 Most Important Features', fontweight='bold', fontsize=12)\n",
    "    ax4.grid(True, alpha=0.3, axis='x')\n",
    "    for i, (bar, value) in enumerate(zip(bars, top15.values)):\n",
    "        ax4.text(value, i, f' {value:.0f}', va='center', fontsize=9)\n",
    "\n",
    "# 5. Prediction Distribution\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "ax5.hist(y_pred_test[y_test.values == 0], bins=50, alpha=0.6, label='Negative Class', edgecolor='black')\n",
    "ax5.hist(y_pred_test[y_test.values == 1], bins=50, alpha=0.6, label='Positive Class', edgecolor='black')\n",
    "ax5.set_xlabel('Predicted Probability')\n",
    "ax5.set_ylabel('Count')\n",
    "ax5.set_title('Prediction Distribution by True Class', fontweight='bold', fontsize=12)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Metrics Comparison (CV vs Holdout)\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "metrics_names = ['ROC-AUC', 'PR-AUC', 'Brier']\n",
    "cv_vals = [cv_results[best_model_name]['roc_auc'], \n",
    "           cv_results[best_model_name]['pr_auc'], \n",
    "           cv_results[best_model_name]['brier']]\n",
    "holdout_vals = [holdout_metrics['roc_auc'], \n",
    "                holdout_metrics['pr_auc'], \n",
    "                holdout_metrics['brier']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "bars1 = ax6.bar(x - width/2, cv_vals, width, label='CV Mean', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax6.bar(x + width/2, holdout_vals, width, label='Holdout', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax6.set_ylabel('Score')\n",
    "ax6.set_title('CV vs Holdout Performance', fontweight='bold', fontsize=12)\n",
    "ax6.set_xticks(x)\n",
    "ax6.set_xticklabels(metrics_names)\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 7. Model Summary Box\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "MODEL SUMMARY\n",
    "{'â”€'*30}\n",
    "\n",
    "Best Model: {best_model_name.upper()}\n",
    "Encoder: {metadata['encoder']}\n",
    "\n",
    "PERFORMANCE (Holdout)\n",
    "  â€¢ ROC-AUC: {holdout_metrics['roc_auc']:.5f}\n",
    "  â€¢ PR-AUC:  {holdout_metrics['pr_auc']:.5f}\n",
    "  â€¢ Brier:   {holdout_metrics['brier']:.5f}\n",
    "\n",
    "TRAINING INFO\n",
    "  â€¢ CV Folds: {CV}\n",
    "  â€¢ Best Iteration: {metadata['best_iteration']}\n",
    "  â€¢ Trees: {metadata['n_trees']}\n",
    "  â€¢ Train Time: {metadata['fit_time_s']:.1f}s\n",
    "  â€¢ Pred Time: {metadata['predict_time_ms_per_1k']:.1f} ms/1k\n",
    "\n",
    "DATASET\n",
    "  â€¢ Train: {len(X_train):,} samples\n",
    "  â€¢ Test: {len(X_test):,} samples\n",
    "  â€¢ Features: {X_train.shape[1]}\n",
    "  â€¢ Imbalance: {y_train.mean()*100:.2f}%\n",
    "\"\"\"\n",
    "\n",
    "ax7.text(0.1, 0.95, summary_text, transform=ax7.transAxes,\n",
    "         fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3, pad=1))\n",
    "\n",
    "# Main title\n",
    "fig.suptitle(f'Porto Seguro Safe Driver Prediction - {best_model_name.upper()} Model Dashboard', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig(REPORTS_OUT / \"plot_comprehensive_dashboard.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ“ Saved comprehensive dashboard: plot_comprehensive_dashboard.png\")\n",
    "print(f\"\\nðŸŽ‰ All done! Check {REPORTS_OUT} for all outputs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "318d2a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENVIRONMENT & REPRODUCIBILITY\n",
      "================================================================================\n",
      "\n",
      "Python: 3.13.3\n",
      "Platform: Darwin 25.1.0\n",
      "\n",
      "Package Versions:\n",
      "  numpy          : 2.3.2\n",
      "  pandas         : 2.3.1\n",
      "  sklearn        : 1.7.1\n",
      "  lightgbm       : 4.6.0\n",
      "  xgboost        : 3.0.4\n",
      "  matplotlib     : 3.10.5\n",
      "  seaborn        : 0.13.2\n",
      "\n",
      "Configuration:\n",
      "  Random Seed: 42\n",
      "  CV Folds: 5\n",
      "  Max Estimators: 4000\n",
      "  Early Stopping: 100\n",
      "\n",
      "âœ“ All runs are reproducible with fixed seed and split indices\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT & REPRODUCIBILITY INFO\n",
    "# ============================================================================\n",
    "\n",
    "import platform\n",
    "import importlib\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ENVIRONMENT & REPRODUCIBILITY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "packages = [\"numpy\", \"pandas\", \"sklearn\", \"lightgbm\", \"xgboost\", \"matplotlib\", \"seaborn\"]\n",
    "versions = {}\n",
    "\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        module = importlib.import_module(pkg)\n",
    "        version = getattr(module, \"__version__\", \"unknown\")\n",
    "    except ImportError:\n",
    "        version = \"not installed\"\n",
    "    versions[pkg] = version\n",
    "\n",
    "print(f\"\\nPython: {platform.python_version()}\")\n",
    "print(f\"Platform: {platform.system()} {platform.release()}\")\n",
    "print(f\"\\nPackage Versions:\")\n",
    "for pkg, ver in versions.items():\n",
    "    print(f\"  {pkg:15s}: {ver}\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Random Seed: {RND}\")\n",
    "print(f\"  CV Folds: {CV}\")\n",
    "print(f\"  Max Estimators: {N_EST}\")\n",
    "print(f\"  Early Stopping: {ESR}\")\n",
    "\n",
    "print(f\"\\nâœ“ All runs are reproducible with fixed seed and split indices\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
